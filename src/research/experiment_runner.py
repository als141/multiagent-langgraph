#!/usr/bin/env python3
"""
å”èª¿çš„å•é¡Œè§£æ±ºå®Ÿé¨“ã‚·ã‚¹ãƒ†ãƒ 

ä¿®å£«ç ”ç©¶ç”¨ã®åŒ…æ‹¬çš„å®Ÿé¨“å®Ÿè¡Œãƒ»åˆ†æãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
å„ç¨®ã‚²ãƒ¼ãƒ ç†è«–æˆ¦ç•¥ã®æ¯”è¼ƒæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 
"""

import asyncio
import json
import os
import time
import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg')  # GUIä¸è¦ã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰è¨­å®š
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
import openai
from openai import AsyncOpenAI
import dotenv
dotenv.load_dotenv()


@dataclass
class ExperimentConfig:
    """å®Ÿé¨“è¨­å®š"""
    experiment_name: str
    num_agents: int = 4
    num_rounds: int = 3
    num_trials: int = 5
    game_types: List[str] = field(default_factory=lambda: ["cooperation", "knowledge_sharing", "evaluation"])
    strategy_combinations: List[List[str]] = field(default_factory=list)
    tasks: List[str] = field(default_factory=lambda: ["remote_work_future", "sustainable_city"])
    output_dir: str = "results/experiments"
    save_detailed_logs: bool = True
    

@dataclass
class ExperimentResult:
    """å®Ÿé¨“çµæœ"""
    experiment_id: str
    config: ExperimentConfig
    timestamp: str
    strategy_performance: Dict[str, Any]
    game_outcomes: List[Dict[str, Any]]
    collaboration_metrics: Dict[str, float]
    solution_quality: Dict[str, float]
    efficiency_metrics: Dict[str, float]
    statistical_analysis: Dict[str, Any]


class ExperimentRunner:
    """å®Ÿé¨“å®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.client = AsyncOpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        self.results_dir = Path("results/experiments")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # æˆ¦ç•¥ã‚¿ã‚¤ãƒ—å®šç¾©
        self.strategy_types = {
            "always_cooperate": "å¸¸ã«å”åŠ›",
            "always_defect": "å¸¸ã«ç«¶äº‰", 
            "tit_for_tat": "å¿œå ±æˆ¦ç•¥",
            "random": "ãƒ©ãƒ³ãƒ€ãƒ ",
            "adaptive": "é©å¿œæˆ¦ç•¥",
            "trust_based": "ä¿¡é ¼ãƒ™ãƒ¼ã‚¹"
        }
        
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå½¹å‰²å®šç¾©
        self.agent_roles = {
            "coordinator": "èª¿æ•´å½¹",
            "analyzer": "åˆ†æå½¹", 
            "creative": "å‰µé€ å½¹",
            "synthesizer": "çµ±åˆå½¹"
        }
    
    async def run_comprehensive_experiment(self, config: ExperimentConfig) -> ExperimentResult:
        """åŒ…æ‹¬çš„å®Ÿé¨“å®Ÿè¡Œ"""
        
        experiment_id = f"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        print(f"ğŸ”¬ åŒ…æ‹¬çš„å®Ÿé¨“é–‹å§‹: {config.experiment_name}")
        print(f"å®Ÿé¨“ID: {experiment_id}")
        print("=" * 70)
        
        start_time = time.time()
        
        # æˆ¦ç•¥çµ„ã¿åˆã‚ã›ç”Ÿæˆ
        if not config.strategy_combinations:
            config.strategy_combinations = self._generate_strategy_combinations(config.num_agents)
        
        all_results = []
        strategy_performance = {}
        
        # å„æˆ¦ç•¥çµ„ã¿åˆã‚ã›ã§å®Ÿé¨“å®Ÿè¡Œ
        for trial in range(config.num_trials):
            print(f"\nğŸ“Š è©¦è¡Œ {trial + 1}/{config.num_trials}")
            
            for i, strategy_combo in enumerate(config.strategy_combinations):
                print(f"  æˆ¦ç•¥çµ„ã¿åˆã‚ã› {i+1}: {strategy_combo}")
                
                # å„ã‚¿ã‚¹ã‚¯ã§å®Ÿé¨“
                for task_name in config.tasks:
                    trial_result = await self._run_single_trial(
                        strategy_combo, task_name, config, trial
                    )
                    
                    if trial_result:
                        all_results.append(trial_result)
                        
                        # æˆ¦ç•¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç´¯ç©
                        combo_key = "_".join(strategy_combo)
                        if combo_key not in strategy_performance:
                            strategy_performance[combo_key] = {
                                "total_quality": 0,
                                "total_cooperation": 0,
                                "total_efficiency": 0,
                                "trial_count": 0
                            }
                        
                        perf = strategy_performance[combo_key]
                        perf["total_quality"] += trial_result.get("solution_quality", 0)
                        perf["total_cooperation"] += trial_result.get("cooperation_level", 0)
                        perf["total_efficiency"] += trial_result.get("efficiency", 0)
                        perf["trial_count"] += 1
        
        # çµ±è¨ˆåˆ†æå®Ÿè¡Œ
        statistical_analysis = self._perform_statistical_analysis(all_results, strategy_performance)
        
        # çµæœé›†ç´„
        experiment_result = ExperimentResult(
            experiment_id=experiment_id,
            config=config,
            timestamp=datetime.now().isoformat(),
            strategy_performance=self._calculate_average_performance(strategy_performance),
            game_outcomes=all_results,
            collaboration_metrics=self._calculate_collaboration_metrics(all_results),
            solution_quality=self._calculate_solution_quality_metrics(all_results),
            efficiency_metrics=self._calculate_efficiency_metrics(all_results),
            statistical_analysis=statistical_analysis
        )
        
        # çµæœä¿å­˜
        await self._save_experiment_results(experiment_result)
        
        # å¯è¦–åŒ–ä½œæˆ
        await self._create_visualizations(experiment_result)
        
        execution_time = time.time() - start_time
        print(f"\nâœ… å®Ÿé¨“å®Œäº†! å®Ÿè¡Œæ™‚é–“: {execution_time:.1f}ç§’")
        
        return experiment_result
    
    def _generate_strategy_combinations(self, num_agents: int) -> List[List[str]]:
        """æˆ¦ç•¥çµ„ã¿åˆã‚ã›ç”Ÿæˆ"""
        strategies = list(self.strategy_types.keys())
        
        # åŸºæœ¬çš„ãªçµ„ã¿åˆã‚ã›ãƒ‘ã‚¿ãƒ¼ãƒ³
        combinations = [
            # åŒè³ªæˆ¦ç•¥
            ["always_cooperate"] * num_agents,
            ["always_defect"] * num_agents,
            ["tit_for_tat"] * num_agents,
            
            # æ··åˆæˆ¦ç•¥
            ["always_cooperate", "always_defect", "tit_for_tat", "adaptive"][:num_agents],
            ["trust_based", "adaptive", "random", "tit_for_tat"][:num_agents],
            ["always_cooperate", "tit_for_tat", "adaptive", "trust_based"][:num_agents],
            
            # å”åŠ›é‡è¦–
            ["always_cooperate", "trust_based", "adaptive", "tit_for_tat"][:num_agents],
            
            # ç«¶äº‰é‡è¦–
            ["always_defect", "random", "tit_for_tat", "adaptive"][:num_agents]
        ]
        
        return combinations[:6]  # å®Ÿé¨“æ™‚é–“çŸ­ç¸®ã®ãŸã‚6ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åˆ¶é™
    
    async def _run_single_trial(self, strategy_combo: List[str], task_name: str, 
                               config: ExperimentConfig, trial_num: int) -> Optional[Dict[str, Any]]:
        """å˜ä¸€è©¦è¡Œå®Ÿè¡Œ"""
        
        print(f"    ã‚¿ã‚¹ã‚¯: {task_name}")
        
        try:
            # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ
            agents = self._create_agents_for_strategies(strategy_combo)
            
            # ã‚¿ã‚¹ã‚¯è¨­å®š
            task_prompt = self._get_task_prompt(task_name)
            
            # å”èª¿çš„å•é¡Œè§£æ±ºã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
            result = await self._simulate_collaborative_solving(
                agents, task_prompt, config.num_rounds
            )
            
            # çµæœã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
            result.update({
                "trial_number": trial_num,
                "task_name": task_name,
                "strategy_combination": strategy_combo,
                "timestamp": datetime.now().isoformat(),
                "llm_execution_confirmed": "all_conversations" in result  # LLMå®Ÿè¡Œç¢ºèª
            })
            
            return result
            
        except Exception as e:
            print(f"      âŒ è©¦è¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _create_agents_for_strategies(self, strategies: List[str]) -> List[Dict[str, Any]]:
        """æˆ¦ç•¥ã«åŸºã¥ãã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ"""
        
        agents = []
        roles = list(self.agent_roles.keys())
        
        for i, strategy in enumerate(strategies):
            role = roles[i % len(roles)]
            
            agent = {
                "agent_id": f"agent_{i+1}",
                "name": f"{self.agent_roles[role]}_{i+1}",
                "role": role,
                "strategy": strategy,
                "personality": self._get_personality_for_strategy(strategy),
                "expertise": self._get_expertise_for_role(role)
            }
            
            agents.append(agent)
        
        return agents
    
    def _get_personality_for_strategy(self, strategy: str) -> Dict[str, float]:
        """æˆ¦ç•¥ã«å¿œã˜ãŸæ€§æ ¼è¨­å®š"""
        
        personalities = {
            "always_cooperate": {
                "cooperation_tendency": 0.95,
                "trust_propensity": 0.9,
                "risk_tolerance": 0.3
            },
            "always_defect": {
                "cooperation_tendency": 0.1,
                "trust_propensity": 0.2,
                "risk_tolerance": 0.8
            },
            "tit_for_tat": {
                "cooperation_tendency": 0.7,
                "trust_propensity": 0.6,
                "risk_tolerance": 0.5
            },
            "random": {
                "cooperation_tendency": 0.5,
                "trust_propensity": 0.5,
                "risk_tolerance": 0.7
            },
            "adaptive": {
                "cooperation_tendency": 0.6,
                "trust_propensity": 0.7,
                "risk_tolerance": 0.4
            },
            "trust_based": {
                "cooperation_tendency": 0.8,
                "trust_propensity": 0.9,
                "risk_tolerance": 0.3
            }
        }
        
        return personalities.get(strategy, personalities["tit_for_tat"])
    
    async def _create_llm_agents(self, agent_configs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å®Ÿéš›ã®LLMã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ"""
        llm_agents = []
        
        for config in agent_configs:
            agent = {
                "agent_id": config["agent_id"],
                "name": config["name"],
                "role": config["role"],
                "strategy": config["strategy"],
                "personality": config["personality"],
                "expertise": config["expertise"],
                "client": self.client,
                "system_prompt": self._create_agent_system_prompt(config)
            }
            llm_agents.append(agent)
        
        return llm_agents
    
    def _create_agent_system_prompt(self, config: Dict[str, Any]) -> str:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç”¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ"""
        strategy_descriptions = {
            "always_cooperate": "å¸¸ã«ä»–è€…ã¨å”åŠ›ã—ã€å…±é€šã®åˆ©ç›Šã‚’é‡è¦–ã™ã‚‹",
            "always_defect": "è‡ªå·±åˆ©ç›Šã‚’æœ€å„ªå…ˆã—ã€ç«¶äº‰çš„ã«è¡Œå‹•ã™ã‚‹",
            "tit_for_tat": "ç›¸æ‰‹ã®è¡Œå‹•ã«å¿œã˜ã¦å¯¾å¿œã‚’å¤‰ãˆã‚‹å¿œå ±æˆ¦ç•¥",
            "random": "çŠ¶æ³ã«å¿œã˜ã¦æŸ”è»Ÿã«åˆ¤æ–­ã™ã‚‹",
            "adaptive": "çµŒé¨“ã‹ã‚‰å­¦ç¿’ã—ã¦æˆ¦ç•¥ã‚’èª¿æ•´ã™ã‚‹",
            "trust_based": "ä¿¡é ¼é–¢ä¿‚ã«åŸºã¥ã„ã¦å”åŠ›åº¦ã‚’æ±ºã‚ã‚‹"
        }
        
        return f"""
ã‚ãªãŸã¯ {config['name']} ã§ã™ã€‚

ã€å½¹å‰²ã€‘: {self.agent_roles[config['role']]}
ã€æˆ¦ç•¥ã€‘: {strategy_descriptions.get(config['strategy'], 'ãƒãƒ©ãƒ³ã‚¹å‹')}
ã€å°‚é–€åˆ†é‡ã€‘: {', '.join(config['expertise'])}
ã€æ€§æ ¼ç‰¹æ€§ã€‘:
- å”åŠ›å‚¾å‘: {config['personality']['cooperation_tendency']:.1f}
- ä¿¡é ¼æ€§å‘: {config['personality']['trust_propensity']:.1f}
- ãƒªã‚¹ã‚¯è¨±å®¹åº¦: {config['personality']['risk_tolerance']:.1f}

ã€è¡Œå‹•æŒ‡é‡ã€‘:
1. ã‚ãªãŸã®å½¹å‰²ã¨æˆ¦ç•¥ã«åŸºã¥ã„ã¦ç™ºè¨€ã—ã¦ãã ã•ã„
2. ä»–ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ã®å”èª¿ã‚’æ„è­˜ã—ã¦ãã ã•ã„
3. å…·ä½“çš„ã§å»ºè¨­çš„ãªææ¡ˆã‚’è¡Œã£ã¦ãã ã•ã„
4. å”åŠ›ã™ã‚‹ã‹ã©ã†ã‹ã‚’æ˜ç¢ºã«è¡¨æ˜ã—ã¦ãã ã•ã„
5. æ—¥æœ¬èªã§è‡ªç„¶ãªä¼šè©±ã‚’è¡Œã£ã¦ãã ã•ã„

ã€å‡ºåŠ›å½¢å¼ã€‘:
ç™ºè¨€å†…å®¹ã‚’è‡ªç„¶ãªæ—¥æœ¬èªã§å‡ºåŠ›ã—ã€æœ€å¾Œã«ã€Œå”åŠ›åº¦: [0.0-1.0ã®æ•°å€¤]ã€ã‚’å«ã‚ã¦ãã ã•ã„ã€‚
"""
    
    async def _conduct_real_llm_collaboration(self, llm_agents: List[Dict[str, Any]], 
                                           task_prompt: str, round_num: int, 
                                           trust_scores: Dict) -> Tuple[List[Dict], List[bool]]:
        """å®Ÿéš›ã®LLMä¼šè©±ã«ã‚ˆã‚‹å”èª¿å®Ÿè¡Œ"""
        conversation = []
        cooperation_decisions = []
        
        # 1. ã‚¿ã‚¹ã‚¯æç¤ºã¨ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³
        discussion_prompt = f"""
ã€ãƒ©ã‚¦ãƒ³ãƒ‰ {round_num + 1}ã€‘

è§£æ±ºã™ã¹ãèª²é¡Œ:
{task_prompt}

ã“ã®èª²é¡Œã«ã¤ã„ã¦ã€ã‚ãªãŸã®å°‚é–€æ€§ã¨æˆ¦ç•¥ã‚’æ´»ã‹ã—ã¦ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ææ¡ˆã—ã€
ä»–ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨å”åŠ›ã—ã¦æœ€é©ãªè§£æ±ºç­–ã‚’è¦‹ã¤ã‘ã¦ãã ã•ã„ã€‚

ä»–ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä¿¡é ¼åº¦:
{self._format_trust_scores(trust_scores, llm_agents[0]['agent_id']) if llm_agents else 'ãªã—'}
"""
        
        # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ç™ºè¨€ã‚’é †æ¬¡å–å¾—
        for i, agent in enumerate(llm_agents):
            # å‰ã®ç™ºè¨€ã‚’æ–‡è„ˆã¨ã—ã¦è¿½åŠ 
            context_messages = []
            if conversation:
                context_messages.append("\n\nã€ã“ã‚Œã¾ã§ã®è­°è«–ã€‘:")
                for j, msg in enumerate(conversation[-3:]):  # ç›´è¿‘3ç™ºè¨€ã‚’å‚ç…§
                    context_messages.append(f"{msg['agent_name']}: {msg['content']}")
            
            full_prompt = discussion_prompt + "\n".join(context_messages)
            
            try:
                # LLM APIå‘¼ã³å‡ºã—
                response = await agent["client"].chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system", "content": agent["system_prompt"]},
                        {"role": "user", "content": full_prompt}
                    ],
                    temperature=0.7,
                    max_tokens=500
                )
                
                content = response.choices[0].message.content
                
                # å”åŠ›åº¦ã‚’æŠ½å‡º
                cooperation_score = self._extract_cooperation_score(content)
                cooperation_decisions.append(cooperation_score > 0.5)
                
                # ä¼šè©±ã«è¿½åŠ 
                conversation.append({
                    "agent_id": agent["agent_id"],
                    "agent_name": agent["name"],
                    "role": agent["role"],
                    "content": content,
                    "cooperation_score": cooperation_score,
                    "timestamp": datetime.now().isoformat()
                })
                
                # LLMå‡ºåŠ›ã‚’ãƒ­ã‚°ã¨ã—ã¦è¡¨ç¤º
                print(f"        {agent['name']}: å”åŠ›åº¦ {cooperation_score:.2f}")
                print(f"        ğŸ’¬ LLMå‡ºåŠ›: \"{content[:150]}{'...' if len(content) > 150 else ''}\"")
                print(f"        ğŸ“Š æˆ¦ç•¥: {agent['strategy']}, å½¹å‰²: {agent['role']}")
                print()
                
            except Exception as e:
                print(f"        âŒ {agent['name']} LLMã‚¨ãƒ©ãƒ¼: {e}")
                print(f"        ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥ã‚’ä½¿ç”¨ã—ã¾ã™")
                print()
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                cooperation_decisions.append(True)
                conversation.append({
                    "agent_id": agent["agent_id"],
                    "agent_name": agent["name"],
                    "role": agent["role"],
                    "content": f"æŠ€è¡“çš„ãªå•é¡Œã«ã‚ˆã‚Šç™ºè¨€ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ï¼ˆæˆ¦ç•¥: {agent['strategy']}ï¼‰",
                    "cooperation_score": 0.5,
                    "timestamp": datetime.now().isoformat()
                })
        
        return conversation, cooperation_decisions
    
    def _format_trust_scores(self, trust_scores: Dict, agent_id: str) -> str:
        """ä¿¡é ¼ã‚¹ã‚³ã‚¢ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        if agent_id not in trust_scores:
            return "ãªã—"
        
        scores = trust_scores[agent_id]
        formatted = []
        for other_id, score in scores.items():
            formatted.append(f"{other_id}: {score:.2f}")
        
        return ", ".join(formatted)
    
    def _extract_cooperation_score(self, content: str) -> float:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å”åŠ›åº¦ã‚’æŠ½å‡º"""
        import re
        
        # ã€Œå”åŠ›åº¦: 0.8ã€ã®ã‚ˆã†ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
        pattern = r'å”åŠ›åº¦[:ï¼š]\s*([0-9]*\.?[0-9]+)'
        match = re.search(pattern, content)
        
        if match:
            try:
                score = float(match.group(1))
                return max(0.0, min(1.0, score))
            except ValueError:
                pass
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã‹ã‚‰æ¨å®š
        cooperation_keywords = ['å”åŠ›', 'é€£æº', 'å”èª¿', 'å…±åŒ', 'ä¸€ç·’', 'æ”¯æ´', 'è³›æˆ']
        competition_keywords = ['ç«¶äº‰', 'åå¯¾', 'ç‹¬ç«‹', 'å€‹åˆ¥', 'å˜ç‹¬', 'æ‰¹åˆ¤']
        
        coop_count = sum(1 for word in cooperation_keywords if word in content)
        comp_count = sum(1 for word in competition_keywords if word in content)
        
        if coop_count > comp_count:
            return 0.7
        elif comp_count > coop_count:
            return 0.3
        else:
            return 0.5
    
    async def _evaluate_solution_quality(self, evaluator_agent: Dict[str, Any], 
                                       conversation: List[Dict], task_prompt: str) -> float:
        """LLMã«ã‚ˆã‚‹è§£æ±ºç­–å“è³ªè©•ä¾¡"""
        
        # ä¼šè©±å†…å®¹ã‚’ã¾ã¨ã‚ã‚‹
        discussion_summary = "\n".join([
            f"{msg['agent_name']}: {msg['content'][:200]}..." 
            for msg in conversation
        ])
        
        evaluation_prompt = f"""
ä»¥ä¸‹ã®èª²é¡Œã«å¯¾ã™ã‚‹è­°è«–ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ï¼š

ã€èª²é¡Œã€‘:
{task_prompt}

ã€è­°è«–å†…å®¹ã€‘:
{discussion_summary}

ä»¥ä¸‹ã®è¦³ç‚¹ã‹ã‚‰0.0-1.0ã§è©•ä¾¡ã—ã€ã€Œè©•ä¾¡: [æ•°å€¤]ã€ã®å½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
1. è§£æ±ºç­–ã®å®Ÿç¾å¯èƒ½æ€§
2. ã‚¢ã‚¤ãƒ‡ã‚¢ã®å‰µé€ æ€§
3. è­°è«–ã®å»ºè¨­æ€§
4. å”åŠ›çš„ãªæ…‹åº¦
"""
        
        try:
            response = await evaluator_agent["client"].chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯å®¢è¦³çš„ãªè©•ä¾¡è€…ã§ã™ã€‚è­°è«–ã®è³ªã‚’å…¬æ­£ã«è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚"},
                    {"role": "user", "content": evaluation_prompt}
                ],
                temperature=0.3,
                max_tokens=200
            )
            
            content = response.choices[0].message.content
            
            print(f"        ğŸ” å“è³ªè©•ä¾¡LLMå‡ºåŠ›: \"{content[:100]}{'...' if len(content) > 100 else ''}\"")
            
            # è©•ä¾¡ã‚¹ã‚³ã‚¢ã‚’æŠ½å‡º
            import re
            pattern = r'è©•ä¾¡[:ï¼š]\s*([0-9]*\.?[0-9]+)'
            match = re.search(pattern, content)
            
            if match:
                score = float(match.group(1))
                print(f"        ğŸ“ˆ æŠ½å‡ºã•ã‚ŒãŸå“è³ªã‚¹ã‚³ã‚¢: {score}")
                return max(0.0, min(1.0, score))
            
        except Exception as e:
            print(f"        è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å”åŠ›åº¦ã«åŸºã¥ãæ¨å®š
        avg_cooperation = np.mean([msg.get('cooperation_score', 0.5) for msg in conversation])
        return avg_cooperation * 0.8 + np.random.random() * 0.2
    
    async def _update_trust_scores_from_conversation(self, trust_scores: Dict, agents: List[Dict], 
                                                   conversation: List[Dict], cooperations: List[bool]):
        """å®Ÿéš›ã®ä¼šè©±å†…å®¹ã«åŸºã¥ãä¿¡é ¼ã‚¹ã‚³ã‚¢æ›´æ–°"""
        
        for i, agent in enumerate(agents):
            agent_id = agent["agent_id"]
            cooperated = cooperations[i]
            
            # å¯¾å¿œã™ã‚‹ä¼šè©±ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ¤œç´¢
            agent_message = next((msg for msg in conversation if msg['agent_id'] == agent_id), None)
            
            for j, other_agent in enumerate(agents):
                if i != j:
                    other_id = other_agent["agent_id"]
                    other_cooperated = cooperations[j]
                    
                    # åŸºæœ¬çš„ãªå”åŠ›è¡Œå‹•ã«ã‚ˆã‚‹èª¿æ•´
                    trust_delta = 0.1 if other_cooperated else -0.05
                    
                    # ä¼šè©±å†…å®¹ã«ã‚ˆã‚‹è¿½åŠ èª¿æ•´
                    if agent_message:
                        content = agent_message['content'].lower()
                        if any(word in content for word in ['ä¿¡é ¼', 'æœŸå¾…', 'é ¼ã‚Šã«']):
                            trust_delta += 0.05
                        elif any(word in content for word in ['ç–‘å•', 'å¿ƒé…', 'ä¸å®‰']):
                            trust_delta -= 0.03
                    
                    current_trust = trust_scores[agent_id][other_id]
                    new_trust = max(0, min(1, current_trust + trust_delta))
                    trust_scores[agent_id][other_id] = new_trust
    
    def _summarize_conversation(self, conversation: List[Dict]) -> str:
        """ä¼šè©±ã®è¦ç´„ä½œæˆ"""
        if not conversation:
            return "ä¼šè©±ãªã—"
        
        summary_parts = []
        for msg in conversation:
            summary_parts.append(f"{msg['agent_name']}: {msg['content'][:100]}...")
        
        return " | ".join(summary_parts)
    
    def _get_expertise_for_role(self, role: str) -> List[str]:
        """å½¹å‰²ã«å¿œã˜ãŸå°‚é–€åˆ†é‡"""
        
        expertise_map = {
            "coordinator": ["ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†", "ãƒãƒ¼ãƒ èª¿æ•´", "æ„æ€æ±ºå®š"],
            "analyzer": ["ãƒ‡ãƒ¼ã‚¿åˆ†æ", "ã‚·ã‚¹ãƒ†ãƒ åˆ†æ", "ãƒªã‚¹ã‚¯è©•ä¾¡"],
            "creative": ["ãƒ‡ã‚¶ã‚¤ãƒ³æ€è€ƒ", "ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³", "å‰µé€ çš„ç™ºæƒ³"],
            "synthesizer": ["ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ", "å…¨ä½“æœ€é©", "ç·åˆåˆ¤æ–­"]
        }
        
        return expertise_map.get(role, ["ä¸€èˆ¬"])
    
    def _get_task_prompt(self, task_name: str) -> str:
        """ã‚¿ã‚¹ã‚¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå–å¾—"""
        
        task_prompts = {
            "remote_work_future": """
ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯æ™‚ä»£ã®æ–°ã—ã„çµ„ç¹”è¨­è¨ˆã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚

èª²é¡Œï¼š
- å¾“æ¥ã®ã‚ªãƒ•ã‚£ã‚¹ä¸­å¿ƒçµ„ç¹”ã‹ã‚‰ã®è„±å´
- åŠ¹ç‡çš„ãªã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³æ–¹æ³•ã®ç¢ºç«‹
- ä¼æ¥­æ–‡åŒ–ã®ç¶­æŒãƒ»ç™ºå±•
- æ–°äººæ•™è‚²ãƒ»OJTã®æ–°æ–¹å¼
- è©•ä¾¡ãƒ»äººäº‹ã‚·ã‚¹ãƒ†ãƒ ã®å¤‰é©

é©æ–°çš„ã§å®Ÿç”¨çš„ãªçµ„ç¹”ãƒ¢ãƒ‡ãƒ«ã‚’è¨­è¨ˆã—ã¦ãã ã•ã„ã€‚
""",
            "sustainable_city": """
2050å¹´ã®æŒç¶šå¯èƒ½ãªæœªæ¥éƒ½å¸‚ã‚’è¨­è¨ˆã—ã¦ãã ã•ã„ã€‚

è¦ä»¶ï¼š
- äººå£50ä¸‡äººè¦æ¨¡
- ç’°å¢ƒè² è·æœ€å°åŒ–
- å†ç”Ÿå¯èƒ½ã‚¨ãƒãƒ«ã‚®ãƒ¼æ´»ç”¨
- ã‚¹ãƒãƒ¼ãƒˆäº¤é€šã‚·ã‚¹ãƒ†ãƒ 
- ç½å®³è€æ€§ã®ç¢ºä¿
- é«˜é½¢åŒ–ç¤¾ä¼šã¸ã®å¯¾å¿œ

åŒ…æ‹¬çš„ã§å®Ÿç¾å¯èƒ½ãªéƒ½å¸‚ãƒ—ãƒ©ãƒ³ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚
""",
            "ai_ethics": """
AIé–‹ç™ºãƒ»é‹ç”¨ã®å€«ç†ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’ç­–å®šã—ã¦ãã ã•ã„ã€‚

å¯¾è±¡æŠ€è¡“ï¼š
- ç”ŸæˆAIãƒ»å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«
- è‡ªå‹•é‹è»¢ã‚·ã‚¹ãƒ†ãƒ 
- åŒ»ç™‚è¨ºæ–­AI
- äººäº‹ãƒ»æ¡ç”¨AI

ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã€ãƒã‚¤ã‚¢ã‚¹ã€é€æ˜æ€§ã€è²¬ä»»ã®è¦³ç‚¹ã‹ã‚‰
å®Ÿç”¨çš„ãªã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
"""
        }
        
        return task_prompts.get(task_name, task_prompts["remote_work_future"])
    
    async def _simulate_collaborative_solving(self, agents: List[Dict[str, Any]], 
                                            task_prompt: str, num_rounds: int) -> Dict[str, Any]:
        """å®Ÿéš›ã®LLMã‚’ä½¿ç”¨ã—ãŸå”èª¿çš„å•é¡Œè§£æ±º"""
        
        # å®Ÿéš›ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ
        llm_agents = await self._create_llm_agents(agents)
        
        cooperation_levels = []
        solution_qualities = []
        trust_scores = {}
        all_conversations = []  # å…¨ä¼šè©±ã‚’ä¿å­˜
        
        # åˆæœŸä¿¡é ¼ã‚¹ã‚³ã‚¢
        for agent in agents:
            trust_scores[agent["agent_id"]] = {}
            for other_agent in agents:
                if agent["agent_id"] != other_agent["agent_id"]:
                    trust_scores[agent["agent_id"]][other_agent["agent_id"]] = 0.5
        
        round_results = []
        
        for round_num in range(num_rounds):
            print(f"      ãƒ©ã‚¦ãƒ³ãƒ‰ {round_num + 1}/{num_rounds} - å®Ÿéš›ã®LLMä¼šè©±å®Ÿè¡Œä¸­...")
            
            # å®Ÿéš›ã®LLMä¼šè©±ã«ã‚ˆã‚‹å”èª¿çš„å•é¡Œè§£æ±º
            round_conversation, round_cooperation = await self._conduct_real_llm_collaboration(
                llm_agents, task_prompt, round_num, trust_scores
            )
            
            # ä¼šè©±ã‚’è¨˜éŒ²
            all_conversations.append({
                "round": round_num + 1,
                "conversation": round_conversation,
                "timestamp": datetime.now().isoformat()
            })
            
            # å”åŠ›ãƒ¬ãƒ™ãƒ«è¨ˆç®—
            cooperation_level = sum(round_cooperation) / len(round_cooperation)
            cooperation_levels.append(cooperation_level)
            
            # LLM ã«ã‚ˆã‚‹è§£æ±ºç­–å“è³ªè©•ä¾¡
            solution_quality = await self._evaluate_solution_quality(
                llm_agents[0], round_conversation, task_prompt
            )
            solution_qualities.append(solution_quality)
            
            # ä¿¡é ¼ã‚¹ã‚³ã‚¢æ›´æ–°ï¼ˆå®Ÿéš›ã®ä¼šè©±å†…å®¹ã«åŸºã¥ãï¼‰
            await self._update_trust_scores_from_conversation(
                trust_scores, agents, round_conversation, round_cooperation
            )
            
            round_results.append({
                "round": round_num + 1,
                "cooperation_level": cooperation_level,
                "solution_quality": solution_quality,
                "individual_cooperation": round_cooperation,
                "conversation_summary": self._summarize_conversation(round_conversation)
            })
            
            print(f"        å®Œäº†: å”åŠ›ãƒ¬ãƒ™ãƒ« {cooperation_level:.3f}, å“è³ª {solution_quality:.3f}")
        
        # å…¨ä½“ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        avg_cooperation = np.mean(cooperation_levels)
        avg_quality = np.mean(solution_qualities)
        final_trust = np.mean([np.mean(list(scores.values())) for scores in trust_scores.values()])
        
        # åŠ¹ç‡æ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        efficiency = (avg_cooperation + avg_quality + final_trust) / 3
        
        return {
            "solution_quality": avg_quality,
            "cooperation_level": avg_cooperation,
            "trust_level": final_trust,
            "efficiency": efficiency,
            "round_results": round_results,
            "final_trust_matrix": trust_scores,
            "convergence_speed": self._calculate_convergence_speed(cooperation_levels),
            "stability": np.std(cooperation_levels),
            "all_conversations": all_conversations,  # å…¨ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚ã‚‹
            "llm_calls_made": len(all_conversations) * len(agents),  # APIå‘¼ã³å‡ºã—å›æ•°
            "total_conversation_length": sum(len(conv["conversation"]) for conv in all_conversations)
        }
    
    def _get_cooperation_probability(self, strategy: str, round_num: int, 
                                   trust_scores: Dict, agent_id: str) -> float:
        """æˆ¦ç•¥ã«åŸºã¥ãå”åŠ›ç¢ºç‡è¨ˆç®—"""
        
        if strategy == "always_cooperate":
            return 0.95
        elif strategy == "always_defect":
            return 0.05
        elif strategy == "random":
            return 0.5
        elif strategy == "tit_for_tat":
            if round_num == 0:
                return 0.8  # åˆå›ã¯å”åŠ›çš„
            else:
                # å‰ãƒ©ã‚¦ãƒ³ãƒ‰ã®ä»–è€…ã®å”åŠ›åº¦ã«åŸºã¥ã
                return 0.6 + np.random.random() * 0.3
        elif strategy == "adaptive":
            # å­¦ç¿’çš„æˆ¦ç•¥
            base_prob = 0.6
            if round_num > 0:
                # æˆåŠŸä½“é¨“ã«åŸºã¥ãèª¿æ•´
                base_prob += (round_num * 0.05)
            return min(0.9, base_prob)
        elif strategy == "trust_based":
            # ä¿¡é ¼åº¦ã«åŸºã¥ã
            if agent_id in trust_scores:
                avg_trust = np.mean(list(trust_scores[agent_id].values()))
                return 0.3 + avg_trust * 0.6
            return 0.6
        
        return 0.5
    
    def _update_trust_scores_simulation(self, trust_scores: Dict, agents: List[Dict], 
                                       cooperations: List[bool]):
        """ä¿¡é ¼ã‚¹ã‚³ã‚¢æ›´æ–°ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
        
        for i, agent in enumerate(agents):
            agent_id = agent["agent_id"]
            cooperated = cooperations[i]
            
            for j, other_agent in enumerate(agents):
                if i != j:
                    other_id = other_agent["agent_id"]
                    other_cooperated = cooperations[j]
                    
                    # ç›¸æ‰‹ã®å”åŠ›è¡Œå‹•ã«åŸºã¥ãä¿¡é ¼åº¦èª¿æ•´
                    trust_delta = 0.1 if other_cooperated else -0.05
                    current_trust = trust_scores[agent_id][other_id]
                    new_trust = max(0, min(1, current_trust + trust_delta))
                    trust_scores[agent_id][other_id] = new_trust
    
    def _calculate_convergence_speed(self, cooperation_levels: List[float]) -> float:
        """åæŸé€Ÿåº¦è¨ˆç®—"""
        if len(cooperation_levels) < 2:
            return 0.0
        
        # å¤‰åŒ–ç‡ã®æ¸›å°‘ã‚’åæŸæŒ‡æ¨™ã¨ã™ã‚‹
        changes = [abs(cooperation_levels[i] - cooperation_levels[i-1]) 
                  for i in range(1, len(cooperation_levels))]
        
        if not changes:
            return 1.0
        
        # å¤‰åŒ–ç‡ãŒå°ã•ã„ã»ã©åæŸãŒé€Ÿã„
        avg_change = np.mean(changes)
        return max(0, 1 - avg_change)
    
    def _calculate_average_performance(self, strategy_performance: Dict) -> Dict[str, Any]:
        """å¹³å‡ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆç®—"""
        
        averaged = {}
        
        for combo, metrics in strategy_performance.items():
            count = metrics["trial_count"]
            if count > 0:
                averaged[combo] = {
                    "avg_quality": metrics["total_quality"] / count,
                    "avg_cooperation": metrics["total_cooperation"] / count,
                    "avg_efficiency": metrics["total_efficiency"] / count,
                    "trial_count": count
                }
        
        return averaged
    
    def _calculate_collaboration_metrics(self, all_results: List[Dict]) -> Dict[str, float]:
        """å”èª¿ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
        
        if not all_results:
            return {}
        
        cooperation_levels = [r.get("cooperation_level", 0) for r in all_results]
        trust_levels = [r.get("trust_level", 0) for r in all_results]
        convergence_speeds = [r.get("convergence_speed", 0) for r in all_results]
        stabilities = [r.get("stability", 1) for r in all_results]
        
        return {
            "avg_cooperation": np.mean(cooperation_levels),
            "std_cooperation": np.std(cooperation_levels),
            "avg_trust": np.mean(trust_levels),
            "std_trust": np.std(trust_levels),
            "avg_convergence_speed": np.mean(convergence_speeds),
            "avg_stability": 1 - np.mean(stabilities)  # å®‰å®šæ€§ã¯å¤‰å‹•ã®é€†
        }
    
    def _calculate_solution_quality_metrics(self, all_results: List[Dict]) -> Dict[str, float]:
        """è§£æ±ºç­–å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
        
        if not all_results:
            return {}
        
        qualities = [r.get("solution_quality", 0) for r in all_results]
        efficiencies = [r.get("efficiency", 0) for r in all_results]
        
        return {
            "avg_quality": np.mean(qualities),
            "std_quality": np.std(qualities),
            "max_quality": np.max(qualities),
            "min_quality": np.min(qualities),
            "avg_efficiency": np.mean(efficiencies),
            "quality_improvement": self._calculate_improvement_trend(qualities)
        }
    
    def _calculate_efficiency_metrics(self, all_results: List[Dict]) -> Dict[str, float]:
        """åŠ¹ç‡æ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
        
        if not all_results:
            return {}
        
        convergence_speeds = [r.get("convergence_speed", 0) for r in all_results]
        stabilities = [r.get("stability", 1) for r in all_results]
        
        return {
            "avg_convergence_speed": np.mean(convergence_speeds),
            "stability_score": 1 - np.mean(stabilities),
            "efficiency_ratio": np.mean([r.get("efficiency", 0) for r in all_results]),
            "performance_consistency": 1 / (1 + np.std([r.get("solution_quality", 0) for r in all_results]))
        }
    
    def _calculate_improvement_trend(self, values: List[float]) -> float:
        """æ”¹å–„å‚¾å‘è¨ˆç®—"""
        if len(values) < 2:
            return 0.0
        
        # ç·šå½¢å›å¸°ã®å‚¾ã
        x = np.arange(len(values))
        y = np.array(values)
        
        if np.std(x) == 0:
            return 0.0
        
        correlation = np.corrcoef(x, y)[0, 1]
        return correlation if not np.isnan(correlation) else 0.0
    
    def _perform_statistical_analysis(self, all_results: List[Dict], 
                                    strategy_performance: Dict) -> Dict[str, Any]:
        """çµ±è¨ˆåˆ†æå®Ÿè¡Œ"""
        
        analysis = {
            "total_trials": len(all_results),
            "strategy_comparison": {},
            "correlation_analysis": {},
            "significance_tests": {}
        }
        
        # æˆ¦ç•¥åˆ¥æ€§èƒ½æ¯”è¼ƒ
        strategy_stats = {}
        for combo, metrics in strategy_performance.items():
            if metrics["trial_count"] > 0:
                strategy_stats[combo] = {
                    "mean_quality": metrics["total_quality"] / metrics["trial_count"],
                    "mean_cooperation": metrics["total_cooperation"] / metrics["trial_count"],
                    "mean_efficiency": metrics["total_efficiency"] / metrics["trial_count"]
                }
        
        analysis["strategy_comparison"] = strategy_stats
        
        # ç›¸é–¢åˆ†æ
        if len(all_results) > 3:
            cooperation_levels = [r.get("cooperation_level", 0) for r in all_results]
            solution_qualities = [r.get("solution_quality", 0) for r in all_results]
            trust_levels = [r.get("trust_level", 0) for r in all_results]
            
            correlations = {}
            correlations["cooperation_quality"] = np.corrcoef(cooperation_levels, solution_qualities)[0, 1]
            correlations["cooperation_trust"] = np.corrcoef(cooperation_levels, trust_levels)[0, 1]
            correlations["trust_quality"] = np.corrcoef(trust_levels, solution_qualities)[0, 1]
            
            # NaNå€¤ã®å‡¦ç†
            for key, value in correlations.items():
                if np.isnan(value):
                    correlations[key] = 0.0
            
            analysis["correlation_analysis"] = correlations
        
        # æœ€å„ªç§€æˆ¦ç•¥ç‰¹å®š
        if strategy_stats:
            best_strategy = max(strategy_stats.keys(), 
                              key=lambda x: strategy_stats[x]["mean_efficiency"])
            analysis["best_strategy"] = {
                "strategy_combination": best_strategy,
                "performance": strategy_stats[best_strategy]
            }
        
        return analysis
    
    async def _save_experiment_results(self, result: ExperimentResult):
        """å®Ÿé¨“çµæœä¿å­˜"""
        
        # JSONå½¢å¼ã§ä¿å­˜
        result_file = self.results_dir / f"{result.experiment_id}_results.json"
        
        # çµæœã‚’è¾æ›¸ã«å¤‰æ›ï¼ˆJSONäº’æ›å½¢å¼ã«å¤‰æ›ï¼‰
        result_dict = {
            "experiment_id": result.experiment_id,
            "config": {
                "experiment_name": result.config.experiment_name,
                "num_agents": result.config.num_agents,
                "num_rounds": result.config.num_rounds,
                "num_trials": result.config.num_trials,
                "strategy_combinations": result.config.strategy_combinations
            },
            "timestamp": result.timestamp,
            "strategy_performance": self._convert_to_json_compatible(result.strategy_performance),
            "collaboration_metrics": self._convert_to_json_compatible(result.collaboration_metrics),
            "solution_quality": self._convert_to_json_compatible(result.solution_quality),
            "efficiency_metrics": self._convert_to_json_compatible(result.efficiency_metrics),
            "statistical_analysis": self._convert_to_json_compatible(result.statistical_analysis),
            "detailed_outcomes": self._convert_to_json_compatible(result.game_outcomes),
            "llm_execution_confirmation": {
                "total_conversations": sum(len(outcome.get('all_conversations', [])) for outcome in result.game_outcomes),
                "total_api_calls": sum(outcome.get('llm_calls_made', 0) for outcome in result.game_outcomes),
                "conversation_data_saved": True,
                "execution_mode": "real_llm_conversations"
            }
        }
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(result_dict, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ çµæœä¿å­˜: {result_file}")
    
    def _convert_to_json_compatible(self, obj):
        """JSONã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³äº’æ›å½¢å¼ã«å¤‰æ›"""
        if isinstance(obj, dict):
            return {key: self._convert_to_json_compatible(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_to_json_compatible(item) for item in obj]
        elif isinstance(obj, (np.bool_, bool)):
            return bool(obj)
        elif isinstance(obj, (np.integer, int)):
            return int(obj)
        elif isinstance(obj, (np.floating, float)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif hasattr(obj, '__dict__'):
            return self._convert_to_json_compatible(obj.__dict__)
        else:
            return obj
    
    async def _create_visualizations(self, result: ExperimentResult):
        """å¯è¦–åŒ–ä½œæˆ (è‹±èªè¡¨ç¤ºãƒ»çœç•¥åä½¿ç”¨)"""
        
        try:
            # è‹±èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
            plt.rcParams['font.family'] = 'DejaVu Sans'
            plt.rcParams['axes.unicode_minus'] = False
            plt.style.use('default')
            fig, axes = plt.subplots(2, 3, figsize=(18, 12))
            
            # å®Ÿé¨“åã‚’è‹±èªã«å¤‰æ›
            experiment_name_en = result.config.experiment_name
            if 'ã‚¯ã‚¤ãƒƒã‚¯' in experiment_name_en:
                experiment_name_en = 'Quick Test'
            elif 'æˆ¦ç•¥æ¯”è¼ƒ' in experiment_name_en:
                experiment_name_en = 'Strategy Comparison'
            elif 'ãƒ†ã‚¹ãƒˆ' in experiment_name_en:
                experiment_name_en = 'LLM Test'
            
            fig.suptitle(f'Experiment Results: {experiment_name_en}', fontsize=16)
            
            # æˆ¦ç•¥åã®çœç•¥ãƒãƒƒãƒ”ãƒ³ã‚°
            strategy_abbrev = {
                'always_cooperate': 'AlwaysCooper',
                'always_defect': 'AlwaysDefect', 
                'tit_for_tat': 'TitForTat',
                'random': 'Random',
                'adaptive': 'Adaptive',
                'trust_based': 'TrustBased'
            }
            
            def abbreviate_strategy_combo(combo_key):
                """æˆ¦ç•¥çµ„ã¿åˆã‚ã›ã‚’çœç•¥"""
                strategies = combo_key.split('_')
                abbreviated = []
                for strategy in strategies:
                    abbreviated.append(strategy_abbrev.get(strategy, strategy[:6]))
                return '+'.join(abbreviated[:2]) + ('...' if len(abbreviated) > 2 else '')
            
            # 1. æˆ¦ç•¥åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ
            ax1 = axes[0, 0]
            strategies = list(result.strategy_performance.keys())
            qualities = [result.strategy_performance[s]["avg_quality"] for s in strategies]
            abbreviated_strategies = [abbreviate_strategy_combo(s) for s in strategies]
            
            ax1.bar(range(len(strategies)), qualities, color='skyblue')
            ax1.set_title('Solution Quality by Strategy')
            ax1.set_xlabel('Strategy Combinations')
            ax1.set_ylabel('Average Quality')
            ax1.set_xticks(range(len(strategies)))
            ax1.set_xticklabels(abbreviated_strategies, rotation=45, ha='right')
            
            # 2. å”åŠ›ãƒ¬ãƒ™ãƒ«æ¯”è¼ƒ
            ax2 = axes[0, 1]
            cooperation_levels = [result.strategy_performance[s]["avg_cooperation"] for s in strategies]
            
            ax2.bar(range(len(strategies)), cooperation_levels, color='lightgreen')
            ax2.set_title('Cooperation Level by Strategy')
            ax2.set_xlabel('Strategy Combinations')
            ax2.set_ylabel('Average Cooperation Level')
            ax2.set_xticks(range(len(strategies)))
            ax2.set_xticklabels(abbreviated_strategies, rotation=45, ha='right')
            
            # 3. åŠ¹ç‡æ€§æ¯”è¼ƒ
            ax3 = axes[0, 2]
            efficiencies = [result.strategy_performance[s]["avg_efficiency"] for s in strategies]
            
            ax3.bar(range(len(strategies)), efficiencies, color='orange')
            ax3.set_title('Efficiency by Strategy')
            ax3.set_xlabel('Strategy Combinations')
            ax3.set_ylabel('Average Efficiency')
            ax3.set_xticks(range(len(strategies)))
            ax3.set_xticklabels(abbreviated_strategies, rotation=45, ha='right')
            
            # 4. ç›¸é–¢åˆ†æ
            ax4 = axes[1, 0]
            if result.statistical_analysis.get("correlation_analysis"):
                corr_data = result.statistical_analysis["correlation_analysis"]
                
                # ç›¸é–¢ãƒ©ãƒ™ãƒ«ã‚’è‹±èªã«å¤‰æ›
                corr_labels_map = {
                    'cooperation_quality': 'Coop-Quality',
                    'cooperation_trust': 'Coop-Trust', 
                    'trust_quality': 'Trust-Quality'
                }
                
                labels = [corr_labels_map.get(k, k) for k in corr_data.keys()]
                values = list(corr_data.values())
                
                colors = ['red' if v < 0 else 'blue' for v in values]
                ax4.bar(labels, values, color=colors)
                ax4.set_title('Correlation Analysis')
                ax4.set_ylabel('Correlation Coefficient')
                ax4.set_ylim(-1, 1)
                ax4.axhline(y=0, color='black', linestyle='-', alpha=0.3)
            
            # 5. å”èª¿ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            ax5 = axes[1, 1]
            collab_metrics = result.collaboration_metrics
            metric_names = ['avg_cooperation', 'avg_trust', 'avg_convergence_speed', 'avg_stability']
            metric_values = [collab_metrics.get(name, 0) for name in metric_names]
            metric_labels = ['Cooperation', 'Trust', 'Convergence', 'Stability']
            
            ax5.bar(metric_labels, metric_values, color='purple', alpha=0.7)
            ax5.set_title('Collaboration Metrics')
            ax5.set_ylabel('Score')
            ax5.set_ylim(0, 1)
            
            # 6. å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹
            ax6 = axes[1, 2]
            quality_metrics = result.solution_quality
            quality_names = ['avg_quality', 'avg_efficiency']
            quality_values = [quality_metrics.get(name, 0) for name in quality_names]
            quality_labels = ['Avg Quality', 'Avg Efficiency']
            
            ax6.bar(quality_labels, quality_values, color='gold', alpha=0.7)
            ax6.set_title('Quality Metrics')
            ax6.set_ylabel('Score')
            ax6.set_ylim(0, 1)
            
            plt.tight_layout()
            
            # ä¿å­˜
            viz_file = self.results_dir / f"{result.experiment_id}_visualization.png"
            plt.savefig(viz_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"ğŸ“Š å¯è¦–åŒ–ä¿å­˜: {viz_file}")
            
        except Exception as e:
            print(f"âš ï¸ å¯è¦–åŒ–ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")


# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿé¨“å®Ÿè¡Œ"""
    
    print("ğŸ”¬ å”èª¿çš„å•é¡Œè§£æ±ºå®Ÿé¨“ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 70)
    
    # API ã‚­ãƒ¼ç¢ºèª
    if not os.getenv('OPENAI_API_KEY'):
        print("âš ï¸ OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œï¼‰")
    
    # å®Ÿé¨“è¨­å®š
    config = ExperimentConfig(
        experiment_name="æˆ¦ç•¥æ¯”è¼ƒå®Ÿé¨“_v1",
        num_agents=4,
        num_rounds=3,
        num_trials=3,  # å®Ÿé¨“æ™‚é–“çŸ­ç¸®ã®ãŸã‚
        tasks=["remote_work_future", "sustainable_city"]
    )
    
    # å®Ÿé¨“å®Ÿè¡Œ
    runner = ExperimentRunner()
    
    try:
        result = await runner.run_comprehensive_experiment(config)
        
        print("\nğŸ‰ å®Ÿé¨“å®Œäº†!")
        print("=" * 70)
        print(f"å®Ÿé¨“ID: {result.experiment_id}")
        print(f"ç·è©¦è¡Œæ•°: {result.statistical_analysis['total_trials']}")
        
        # ä¸»è¦çµæœè¡¨ç¤º
        if "best_strategy" in result.statistical_analysis:
            best = result.statistical_analysis["best_strategy"]
            print(f"\nğŸ† æœ€å„ªç§€æˆ¦ç•¥çµ„ã¿åˆã‚ã›:")
            print(f"  æˆ¦ç•¥: {best['strategy_combination']}")
            print(f"  åŠ¹ç‡æ€§: {best['performance']['mean_efficiency']:.3f}")
            print(f"  å”åŠ›ãƒ¬ãƒ™ãƒ«: {best['performance']['mean_cooperation']:.3f}")
            print(f"  è§£æ±ºç­–å“è³ª: {best['performance']['mean_quality']:.3f}")
        
        # å”èª¿ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        collab = result.collaboration_metrics
        print(f"\nğŸ“Š å…¨ä½“å”èª¿ãƒ¡ãƒˆãƒªã‚¯ã‚¹:")
        print(f"  å¹³å‡å”åŠ›ãƒ¬ãƒ™ãƒ«: {collab.get('avg_cooperation', 0):.3f}")
        print(f"  å¹³å‡ä¿¡é ¼ãƒ¬ãƒ™ãƒ«: {collab.get('avg_trust', 0):.3f}")
        print(f"  åæŸé€Ÿåº¦: {collab.get('avg_convergence_speed', 0):.3f}")
        print(f"  å®‰å®šæ€§: {collab.get('avg_stability', 0):.3f}")
        
        # ç›¸é–¢åˆ†æ
        if "correlation_analysis" in result.statistical_analysis:
            corr = result.statistical_analysis["correlation_analysis"]
            print(f"\nğŸ”— ç›¸é–¢åˆ†æ:")
            print(f"  å”åŠ›-å“è³ªç›¸é–¢: {corr.get('cooperation_quality', 0):.3f}")
            print(f"  å”åŠ›-ä¿¡é ¼ç›¸é–¢: {corr.get('cooperation_trust', 0):.3f}")
            print(f"  ä¿¡é ¼-å“è³ªç›¸é–¢: {corr.get('trust_quality', 0):.3f}")
        
        print(f"\nğŸ“ è©³ç´°çµæœ: results/experiments/")
        
        return result
        
    except Exception as e:
        print(f"âŒ å®Ÿé¨“ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    asyncio.run(main())