# LangGraph Multi-Agent System é–‹ç™ºè¨˜éŒ²

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦

ä¿®å£«ç ”ç©¶ã€Œé€²åŒ–çš„ç¾¤çŸ¥èƒ½ã«åŸºã¥ãLoRAã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé›†å›£ã®å”èª¿çš„æœ€é©åŒ–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€ã®ä¸€ç’°ã¨ã—ã¦ã€LangGraphã‚’ä½¿ç”¨ã—ãŸãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ ã‚’é–‹ç™ºã—ã¦ã„ã¾ã™ã€‚

## ğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆ

### multiagent-langgraph/
LangGraphãƒ™ãƒ¼ã‚¹ã®ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ 
- **åŸºç›¤ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**: LangGraphã«ã‚ˆã‚‹çŠ¶æ…‹ç®¡ç†ãƒ»ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼åˆ¶å¾¡
- **ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…**: ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹æˆ¦ç•¥ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
- **å®Ÿé¨“ç’°å¢ƒ**: ã‚²ãƒ¼ãƒ ç†è«–å®Ÿé¨“ãƒ»åˆ†æã‚¨ãƒ³ã‚¸ãƒ³

### openai-multiagent/
OpenAI LLMãƒ™ãƒ¼ã‚¹ã®çœŸã®AIä¼šè©±ã‚·ã‚¹ãƒ†ãƒ 
- **LLMçµ±åˆ**: GPT-4o-miniã«ã‚ˆã‚‹è‡ªç„¶è¨€èªæ¨è«–
- **çœŸã®å¯¾è©±**: æ—¥æœ¬èªã§ã®æˆ¦ç•¥çš„ä¼šè©±å®Ÿé¨“
- **é«˜åº¦åˆ†æ**: æ¨è«–éç¨‹ãƒ»æ„Ÿæƒ…çŠ¶æ…‹ã®æ•°å€¤åŒ–è¿½è·¡

## ğŸ¯ é–‹ç™ºæˆæœ

### âœ… å®Œæˆæ©Ÿèƒ½

#### 1. åŸºç›¤ã‚·ã‚¹ãƒ†ãƒ ï¼ˆä¸¡ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…±é€šï¼‰
- **ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç®¡ç†**: å‹•çš„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆãƒ»ç™»éŒ²
- **æˆ¦ç•¥å®Ÿè£…**: å”åŠ›ãƒ»ç«¶äº‰ãƒ»TitForTatãƒ»é©å¿œæˆ¦ç•¥
- **ã‚²ãƒ¼ãƒ ç†è«–**: å›šäººã®ã‚¸ãƒ¬ãƒ³ãƒãƒ»çŸ¥è­˜å…±æœ‰ã‚²ãƒ¼ãƒ 
- **å®Ÿé¨“åˆ¶å¾¡**: è¨­å®šå¯èƒ½ãªå®Ÿé¨“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- **ãƒ‡ãƒ¼ã‚¿åé›†**: è©³ç´°ãªå®Ÿé¨“çµæœè¨˜éŒ²

#### 2. LangGraphã‚·ã‚¹ãƒ†ãƒ ï¼ˆmultiagent-langgraph/ï¼‰
- **ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç®¡ç†**: LangGraphã«ã‚ˆã‚‹çŠ¶æ…‹é·ç§»åˆ¶å¾¡
- **ãƒ¡ãƒ¢ãƒªã‚·ã‚¹ãƒ†ãƒ **: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨˜æ†¶ãƒ»å­¦ç¿’æ©Ÿèƒ½
- **å”èª¿ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“é€£æºæ©Ÿèƒ½
- **åˆ†æã‚¨ãƒ³ã‚¸ãƒ³**: å®Ÿé¨“çµæœã®çµ±è¨ˆåˆ†æãƒ»å¯è¦–åŒ–

#### 3. OpenAI LLMã‚·ã‚¹ãƒ†ãƒ ï¼ˆopenai-multiagent/ï¼‰
- **çœŸã®AIå¯¾è©±**: GPT-4o-miniã«ã‚ˆã‚‹è‡ªç„¶è¨€èªä¼šè©±
- **æ—¥æœ¬èªæ¨è«–**: æ—¥æœ¬èªã§ã®æˆ¦ç•¥çš„æ€è€ƒãƒ»æ„æ€æ±ºå®š
- **æ„Ÿæƒ…ãƒ¢ãƒ‡ãƒ«**: ä¿¡é ¼åº¦ãƒ»å”åŠ›å¯èƒ½æ€§ã®æ•°å€¤åŒ–
- **å­¦ç¿’æ©Ÿèƒ½**: éå»çµŒé¨“ã‹ã‚‰ã®æˆ¦ç•¥èª¿æ•´

### ğŸ§ª å®Ÿè¨¼æ¸ˆã¿å®Ÿé¨“

#### OpenAI LLMå®Ÿé¨“æˆæœ
```
ğŸ”¬ å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ï¼š
- APIå‘¼ã³å‡ºã—: 10å›æˆåŠŸï¼ˆHTTP 200 OKï¼‰
- ãƒ¢ãƒ‡ãƒ«: GPT-4o-mini
- å®Ÿè¡Œæ™‚é–“: ç´„45ç§’
- è¨€èª: å®Œå…¨æ—¥æœ¬èªå¯¾å¿œ

ğŸ’¬ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥ç‰¹æ€§ï¼š
- å¤–äº¤å®˜_ç”°ä¸­: ç¤¼å„€æ­£ã—ã„é•·æœŸé–¢ä¿‚é‡è¦–
- æ¥½è¦³ä¸»ç¾©è€…_ä½è—¤: å‰å‘ããªå…¨é¢å”åŠ›å¿—å‘
- æˆ¦ç•¥å®¶_éˆ´æœ¨: å†·é™ãªè‡ªå·±åˆ©ç›Šè¿½æ±‚
- é©å¿œè€…_å±±ç”°: åˆ†æçš„ãªçŠ¶æ³é©å¿œ

ğŸ“Š å®šé‡çµæœï¼š
- å”åŠ›å¯èƒ½æ€§: 0.30-1.00
- ä¿¡é ¼å¤‰åŒ–: -0.20ï½+0.50
- æˆ¦ç•¥çš„å­¦ç¿’: å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒçµŒé¨“å­¦ç¿’
```

### ğŸ“‹ ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†ä½“ç³»

#### .gitignoreè¨­å®šå®Œäº†
ä¸¡ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«åŒ…æ‹¬çš„ãª`.gitignore`ã‚’è¨­å®šï¼š
- **Pythoné–¢é€£**: __pycache__, *.pyc, ä»®æƒ³ç’°å¢ƒç­‰
- **å®Ÿé¨“çµæœ**: results/, *_results.json, ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ç­‰
- **è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«**: .env, APIã‚­ãƒ¼, èªè¨¼æƒ…å ±ç­‰
- **IDEè¨­å®š**: .vscode/, .idea/, OSå›ºæœ‰ãƒ•ã‚¡ã‚¤ãƒ«ç­‰
- **ç ”ç©¶å›ºæœ‰**: LLMå‡ºåŠ›, ä¼šè©±å±¥æ­´, åˆ†æã‚­ãƒ£ãƒƒã‚·ãƒ¥ç­‰

## ğŸš€ ã‚·ã‚¹ãƒ†ãƒ ä½¿ç”¨æ–¹æ³•

### ğŸ“‹ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã‚¬ã‚¤ãƒ‰

#### 1. åŸºæœ¬ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# 1. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ç§»å‹•
cd /home/als0028/work/research/multiagent-langgraph

# 2. ä»®æƒ³ç’°å¢ƒã®æœ‰åŠ¹åŒ–
source .venv/bin/activate

# 3. ç’°å¢ƒå¤‰æ•°ã®ç¢ºèª
cat .env
# OPENAI_API_KEY=your_api_key_here

# 4. ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ç¢ºèª
python -c "import openai; print('OpenAI APIæ¥ç¶šOK')"
```

#### 2. å®Ÿè£…æ¸ˆã¿ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè¡Œ

##### A. æ—¥æœ¬èªLLMå®Ÿé¨“ï¼ˆå®Ÿè¨¼æ¸ˆã¿ï¼‰
```bash
# åŸºæœ¬å®Ÿé¨“ï¼ˆ4ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€å›šäººã®ã‚¸ãƒ¬ãƒ³ãƒï¼‰
python japanese_llm_experiment.py

# å‡ºåŠ›ä¾‹ï¼š
# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ: å¤–äº¤å®˜_ç”°ä¸­, æ¥½è¦³ä¸»ç¾©è€…_ä½è—¤, æˆ¦ç•¥å®¶_éˆ´æœ¨, é©å¿œè€…_å±±ç”°
# APIå‘¼ã³å‡ºã—æˆåŠŸ: 10/10
# å®Ÿè¡Œæ™‚é–“: 45ç§’
# å”åŠ›å¯èƒ½æ€§: 0.30-1.00ï¼ˆå‹•çš„å¤‰åŒ–ï¼‰
```

##### B. é«˜åº¦ã‚²ãƒ¼ãƒ ç†è«–å®Ÿé¨“
```bash
# å…¬å…±è²¡ã‚²ãƒ¼ãƒ å®Ÿé¨“
python src/experiments/advanced_game_experiments.py

# ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§ã®å®Ÿè¡Œ
python -c "
from src.experiments.advanced_game_experiments import *
config = ExperimentConfig(
    name='custom_experiment',
    num_agents=6,
    num_rounds=20,
    games_to_test=[GameType.PUBLIC_GOODS, GameType.TRUST_GAME]
)
suite = AdvancedGameExperimentSuite(config)
results = suite.run_comprehensive_experiment()
print(f'å®Ÿé¨“å®Œäº†: {len(results)}çµæœ')
"
```

##### C. åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
```bash
# å…¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œ
python src/experiments/integrated_benchmark_system.py

# ç‰¹å®šãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
python -c "
from src.experiments.integrated_benchmark_system import *
import asyncio

async def run_basic_benchmark():
    benchmark = IntegratedBenchmarkSystem()
    results = await benchmark.run_benchmark_suite('basic_games')
    print(f'ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†: {len(results)}ã‚¿ã‚¹ã‚¯')
    return results

asyncio.run(run_basic_benchmark())
"
```

##### D. Responses APIçµ±åˆï¼ˆè¨­è¨ˆæ¸ˆã¿ï¼‰
```bash
# Responses APIãƒ‡ãƒ¢
python src/experiments/responses_api_demo.py

# æ³¨æ„: å®Ÿéš›ã®Responses APIãŒåˆ©ç”¨å¯èƒ½ã«ãªã£ãŸã‚‰å®Ÿè¡Œå¯èƒ½
```

### ğŸ® è©³ç´°å®Ÿé¨“æ‰‹é †

#### 1. LLMã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿé¨“

##### å˜ä¸€ã‚²ãƒ¼ãƒ å®Ÿé¨“
```python
# src/experiments/custom_single_game.py ã¨ã—ã¦ä½œæˆ
from src.multiagent_system.agents.llm_game_agent import LLMGameAgent
from src.multiagent_system.game_theory.advanced_games import PublicGoodsGame
import asyncio

async def single_game_experiment():
    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ
    agents = [
        LLMGameAgent("å”åŠ›è€…", {"cooperation_tendency": 0.9}),
        LLMGameAgent("ç«¶äº‰è€…", {"cooperation_tendency": 0.3}),
        LLMGameAgent("æˆ¦ç•¥å®¶", {"cooperation_tendency": 0.6})
    ]
    
    # ã‚²ãƒ¼ãƒ ä½œæˆ
    game = PublicGoodsGame(num_players=3, multiplier=2.5, endowment=100.0)
    
    # å®Ÿè¡Œ
    agent_ids = [agent.agent_id for agent in agents]
    state = game.initialize(agent_ids)
    
    # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ„æ€æ±ºå®š
    for agent in agents:
        info_set = game.get_information_set(agent.agent_id, state)
        action, reasoning = await agent.make_decision(game, state, info_set)
        print(f"{agent.agent_id}: {action.action_type} = {action.value}")
        print(f"æ¨è«–: {reasoning.decision_rationale}")
    
    return state

# å®Ÿè¡Œ
asyncio.run(single_game_experiment())
```

#### 2. çŸ¥è­˜äº¤æ›å®Ÿé¨“

```python
# çŸ¥è­˜ãƒãƒ¼ã‚±ãƒƒãƒˆå®Ÿé¨“
from src.multiagent_system.knowledge.knowledge_exchange_system import KnowledgeMarket, KnowledgeItem, KnowledgeType
from datetime import datetime

# çŸ¥è­˜ãƒãƒ¼ã‚±ãƒƒãƒˆä½œæˆ
market = KnowledgeMarket()

# çŸ¥è­˜ã‚¢ã‚¤ãƒ†ãƒ è¿½åŠ 
knowledge1 = KnowledgeItem(
    id="",
    content="å”åŠ›æˆ¦ç•¥ã¯é•·æœŸçš„ã«ã¯åˆ©ç›Šã‚’ã‚‚ãŸã‚‰ã™",
    knowledge_type=KnowledgeType.STRATEGIC,
    source_agent="expert_agent",
    created_at=datetime.now(),
    topic="cooperation_strategy",
    confidence=0.9,
    utility_value=0.8
)

market.add_knowledge(knowledge1)

# çŸ¥è­˜æ¤œç´¢
results = market.search_knowledge("å”åŠ›", KnowledgeType.STRATEGIC, "seeker_agent")
print(f"æ¤œç´¢çµæœ: {len(results)}ä»¶")
for result in results:
    print(f"- {result.content} (ä¿¡é ¼åº¦: {result.confidence})")
```

#### 3. ä¿¡é ¼ãƒ»è©•åˆ¤ã‚·ã‚¹ãƒ†ãƒ å®Ÿé¨“

```python
# ä¿¡é ¼ã‚·ã‚¹ãƒ†ãƒ å®Ÿé¨“
from src.multiagent_system.reputation.trust_reputation_system import TrustReputationSystem, InteractionType

# ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
trust_system = TrustReputationSystem()

# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç™»éŒ²
agents = ["Alice", "Bob", "Charlie"]
for agent in agents:
    trust_system.register_agent(agent)

# ç›¸äº’ä½œç”¨è¨˜éŒ²
interaction_id = trust_system.record_interaction(
    agent_a="Alice",
    agent_b="Bob", 
    interaction_type=InteractionType.COOPERATION,
    outcome="success",
    details={"payoff_a": 10, "payoff_b": 10},
    satisfaction_a=0.9,
    satisfaction_b=0.8,
    context="public_goods_game"
)

# ä¿¡é ¼ã‚¹ã‚³ã‚¢å–å¾—
trust_score = trust_system.get_trust_score("Alice", "Bob")
print(f"Aliceâ†’Bobã®ä¿¡é ¼åº¦: {trust_score.overall:.3f}")

# è©•åˆ¤ã‚¹ã‚³ã‚¢å–å¾—
reputation = trust_system.get_reputation_score("Bob")
print(f"Bobã®è©•åˆ¤: {reputation:.3f}")
```

### ğŸ“Š å®Ÿé¨“çµæœã®è§£æ

#### 1. çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®å ´æ‰€
```bash
# å®Ÿé¨“çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
ls results/

# å…·ä½“çš„ãªçµæœãƒ•ã‚¡ã‚¤ãƒ«ä¾‹
ls results/advanced_experiments/
ls results/benchmarks/
ls results/sample_advanced/
```

#### 2. çµæœãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿

```python
# å®Ÿé¨“çµæœã®åˆ†æ
import json
import pandas as pd
import matplotlib.pyplot as plt

# å®Ÿé¨“çµæœèª­ã¿è¾¼ã¿
with open('results/advanced_experiments/public_goods_results.json', 'r') as f:
    results = json.load(f)

# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ åŒ–
outcomes = results['outcomes']
df = pd.DataFrame([
    {
        'round': i,
        'social_welfare': outcome['social_welfare'],
        'cooperation_level': outcome['cooperation_level'],
        'fairness_index': outcome['fairness_index']
    }
    for i, outcome in enumerate(outcomes)
])

# å¯è¦–åŒ–
plt.figure(figsize=(12, 4))

plt.subplot(131)
plt.plot(df['round'], df['social_welfare'])
plt.title('ç¤¾ä¼šåšç”Ÿã®æ¨ç§»')
plt.xlabel('ãƒ©ã‚¦ãƒ³ãƒ‰')
plt.ylabel('ç¤¾ä¼šåšç”Ÿ')

plt.subplot(132) 
plt.plot(df['round'], df['cooperation_level'])
plt.title('å”åŠ›ãƒ¬ãƒ™ãƒ«ã®æ¨ç§»')
plt.xlabel('ãƒ©ã‚¦ãƒ³ãƒ‰')
plt.ylabel('å”åŠ›ãƒ¬ãƒ™ãƒ«')

plt.subplot(133)
plt.plot(df['round'], df['fairness_index'])
plt.title('å…¬å¹³æ€§æŒ‡æ•°ã®æ¨ç§»')
plt.xlabel('ãƒ©ã‚¦ãƒ³ãƒ‰')
plt.ylabel('å…¬å¹³æ€§æŒ‡æ•°')

plt.tight_layout()
plt.savefig('results/analysis_summary.png')
plt.show()
```

#### 3. ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã®ç¢ºèª

```python
# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœåˆ†æ
from src.experiments.integrated_benchmark_system import IntegratedBenchmarkSystem

benchmark = IntegratedBenchmarkSystem()
summary = benchmark.get_benchmark_summary()

print("åˆ©ç”¨å¯èƒ½ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆ:")
for suite_name, details in summary["suite_details"].items():
    print(f"- {suite_name}: {details['description']}")
    print(f"  ã‚¿ã‚¹ã‚¯æ•°: {details['task_count']}")
    print(f"  äºˆæƒ³æ™‚é–“: {details['estimated_time_minutes']}åˆ†")
    print(f"  è¤‡é›‘åº¦: {details['complexity_range']}")
```

### ğŸ”§ ã‚«ã‚¹ã‚¿ãƒ å®Ÿé¨“ã®ä½œæˆ

#### 1. æ–°ã—ã„ã‚²ãƒ¼ãƒ ã‚¿ã‚¤ãƒ—ã®å®Ÿè£…

```python
# src/multiagent_system/game_theory/custom_game.py
from src.multiagent_system.game_theory.advanced_games import AdvancedGame, GameType, Action, GameState, GameOutcome

class CustomCooperationGame(AdvancedGame):
    def __init__(self, num_players: int, **kwargs):
        super().__init__(GameType.COORDINATION, num_players, **kwargs)
        self.cooperation_threshold = kwargs.get("cooperation_threshold", 0.5)
    
    def initialize(self, players: List[str]) -> GameState:
        return GameState(
            players=players,
            public_info={"cooperation_count": 0},
            private_info={p: {"chosen_action": None} for p in players}
        )
    
    def is_valid_action(self, action: Action, state: GameState) -> bool:
        return action.action_type in ["cooperate", "defect"]
    
    def apply_action(self, action: Action, state: GameState) -> GameState:
        new_state = state.model_copy(deep=True)
        new_state.private_info[action.agent_id]["chosen_action"] = action.action_type
        
        if action.action_type == "cooperate":
            new_state.public_info["cooperation_count"] += 1
            
        # å…¨å“¡ãŒé¸æŠã—ãŸã‹ãƒã‚§ãƒƒã‚¯
        if all(info["chosen_action"] for info in new_state.private_info.values()):
            new_state.terminated = True
            
        return new_state
    
    def calculate_payoffs(self, state: GameState) -> Dict[str, float]:
        cooperation_count = state.public_info["cooperation_count"]
        total_players = len(state.players)
        cooperation_rate = cooperation_count / total_players
        
        # å”åŠ›é–¾å€¤ã‚’è¶…ãˆãŸå ´åˆã€å…¨å“¡ã«ãƒœãƒ¼ãƒŠã‚¹
        base_payoff = 10 if cooperation_rate >= self.cooperation_threshold else 5
        
        payoffs = {}
        for player, info in state.private_info.items():
            if info["chosen_action"] == "cooperate":
                payoffs[player] = base_payoff + 2  # å”åŠ›ãƒœãƒ¼ãƒŠã‚¹
            else:
                payoffs[player] = base_payoff - 1  # éå”åŠ›ãƒšãƒŠãƒ«ãƒ†ã‚£
                
        return payoffs
    
    def is_terminal(self, state: GameState) -> bool:
        return state.terminated
```

#### 2. ã‚«ã‚¹ã‚¿ãƒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ€§æ ¼ã®ä½œæˆ

```python
# ã‚«ã‚¹ã‚¿ãƒ æ€§æ ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
custom_personalities = {
    "æ…é‡ãªå”åŠ›è€…": {
        "cooperation_tendency": 0.8,
        "risk_tolerance": 0.2,
        "trust_propensity": 0.7,
        "rationality": 0.9,
        "learning_speed": 0.3,
        "communication_style": "cautious",
        "description": "æ…é‡ã ãŒå”åŠ›çš„ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"
    },
    "ç©æ¥µçš„ç«¶äº‰è€…": {
        "cooperation_tendency": 0.2,
        "risk_tolerance": 0.9,
        "trust_propensity": 0.3,
        "rationality": 0.8,
        "learning_speed": 0.7,
        "communication_style": "aggressive",
        "description": "ç©æ¥µçš„ã§ç«¶äº‰çš„ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"
    },
    "ãƒãƒ©ãƒ³ã‚¹å‹å­¦ç¿’è€…": {
        "cooperation_tendency": 0.5,
        "risk_tolerance": 0.5,
        "trust_propensity": 0.5,
        "rationality": 0.9,
        "learning_speed": 0.8,
        "communication_style": "adaptive",
        "description": "çŠ¶æ³ã«å¿œã˜ã¦å­¦ç¿’ãƒ»é©å¿œã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"
    }
}

# ä½¿ç”¨ä¾‹
from src.multiagent_system.agents.llm_game_agent import LLMGameAgent

agents = [
    LLMGameAgent("agent_1", custom_personalities["æ…é‡ãªå”åŠ›è€…"]),
    LLMGameAgent("agent_2", custom_personalities["ç©æ¥µçš„ç«¶äº‰è€…"]),
    LLMGameAgent("agent_3", custom_personalities["ãƒãƒ©ãƒ³ã‚¹å‹å­¦ç¿’è€…"])
]
```

### ğŸ“ˆ é«˜åº¦ãªåˆ†æãƒ»å¯è¦–åŒ–

#### 1. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¡Œå‹•åˆ†æ

```python
# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¡Œå‹•ã®è©³ç´°åˆ†æ
import seaborn as sns
import numpy as np

def analyze_agent_behavior(experiment_results):
    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥ã®è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
    agent_data = []
    
    for result in experiment_results:
        for agent_id, performance in result.agent_performances.items():
            agent_data.append({
                'agent_id': agent_id,
                'cooperation_rate': performance.get('cooperation_rate', 0),
                'average_payoff': performance.get('average_payoff', 0),
                'trust_given': performance.get('trust_given', 0),
                'trust_received': performance.get('trust_received', 0)
            })
    
    df = pd.DataFrame(agent_data)
    
    # ç›¸é–¢åˆ†æ
    correlation_matrix = df[['cooperation_rate', 'average_payoff', 'trust_given', 'trust_received']].corr()
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
    plt.title('ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¡Œå‹•æŒ‡æ¨™ã®ç›¸é–¢é–¢ä¿‚')
    plt.tight_layout()
    plt.savefig('results/agent_behavior_correlation.png')
    plt.show()
    
    return df, correlation_matrix

# ä½¿ç”¨ä¾‹
# df, corr = analyze_agent_behavior(experiment_results)
```

#### 2. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æ

```python
# ä¿¡é ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å¯è¦–åŒ–
import networkx as nx

def visualize_trust_network(trust_system):
    # ä¿¡é ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å–å¾—
    network_metrics = trust_system.get_trust_network_metrics()
    G = trust_system.trust_network
    
    plt.figure(figsize=(12, 8))
    
    # ãƒãƒ¼ãƒ‰ã‚µã‚¤ã‚ºã‚’è©•åˆ¤ã‚¹ã‚³ã‚¢ã«åŸºã¥ã„ã¦è¨­å®š
    node_sizes = []
    for node in G.nodes():
        reputation = trust_system.get_reputation_score(node)
        node_sizes.append(reputation * 1000)
    
    # ã‚¨ãƒƒã‚¸ã®å¤ªã•ã‚’ä¿¡é ¼åº¦ã«åŸºã¥ã„ã¦è¨­å®š
    edge_weights = []
    for u, v, data in G.edges(data=True):
        edge_weights.append(data.get('weight', 0.5) * 5)
    
    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æç”»
    pos = nx.spring_layout(G)
    nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color='lightblue', alpha=0.7)
    nx.draw_networkx_labels(G, pos, font_size=8)
    nx.draw_networkx_edges(G, pos, width=edge_weights, alpha=0.6, edge_color='gray')
    
    plt.title(f'ä¿¡é ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (å¯†åº¦: {network_metrics.get("network_density", 0):.3f})')
    plt.axis('off')
    plt.tight_layout()
    plt.savefig('results/trust_network.png')
    plt.show()

# ä½¿ç”¨ä¾‹
# visualize_trust_network(trust_system)
```

### ğŸš€ ä»Šå¾Œã®é–‹ç™ºè¨ˆç”»

### Phase 1: ã‚·ã‚¹ãƒ†ãƒ æ‹¡å¼µï¼ˆçŸ­æœŸï¼‰

#### 1.1 LangGraphã‚·ã‚¹ãƒ†ãƒ å¼·åŒ–
- [ ] **è¤‡é›‘ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼**: ã‚ˆã‚Šé«˜åº¦ãªçŠ¶æ…‹é·ç§»ãƒ‘ã‚¿ãƒ¼ãƒ³
- [ ] **ä¸¦åˆ—å‡¦ç†**: å¤§è¦æ¨¡ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç¾¤ã®åŠ¹ç‡çš„ç®¡ç†
- [ ] **æ°¸ç¶šåŒ–**: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçŠ¶æ…‹ãƒ»å­¦ç¿’å±¥æ­´ã®é•·æœŸä¿å­˜
- [ ] **åˆ†æ•£å‡¦ç†**: ã‚¯ãƒ©ã‚¹ã‚¿ç’°å¢ƒã§ã®å®Ÿé¨“å®Ÿè¡Œ

#### 1.2 LLMã‚·ã‚¹ãƒ†ãƒ é«˜åº¦åŒ–
- [ ] **å¤šè¨€èªå¯¾å¿œ**: è‹±èªãƒ»ä¸­å›½èªã§ã®å®Ÿé¨“æ‹¡å¼µ
- [ ] **GPT-4çµ±åˆ**: ã‚ˆã‚Šé«˜åº¦ãªæ¨è«–èƒ½åŠ›æ´»ç”¨
- [ ] **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–**: æˆ¦ç•¥åˆ¥æœ€é©åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé–‹ç™º
- [ ] **ä¼šè©±åˆ†æ**: æ„Ÿæƒ…ãƒ»ãƒˆãƒ¼ãƒ³ãƒ»æ„å›³ã®è©³ç´°è§£æ

#### 1.3 å®Ÿé¨“æ©Ÿèƒ½æ‹¡å¼µ
- [ ] **æ–°ã‚²ãƒ¼ãƒ å®Ÿè£…**: 
  - å…¬å…±è²¡ã‚²ãƒ¼ãƒ 
  - ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³ç†è«–
  - ä¿¡é ¼ã‚²ãƒ¼ãƒ 
  - ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å½¢æˆã‚²ãƒ¼ãƒ 
- [ ] **å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**: å®Ÿé¨“ä¸­ã®æ¡ä»¶å¤‰æ›´
- [ ] **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æ**: ãƒ©ã‚¤ãƒ–å®Ÿé¨“ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

### Phase 2: ç ”ç©¶çµ±åˆï¼ˆä¸­æœŸï¼‰

#### 2.1 LoRAã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçµ±åˆ
- [ ] **LoRAå®Ÿè£…**: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
- [ ] **çŸ¥è­˜è’¸ç•™**: å¤§è¦æ¨¡â†’å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®çŸ¥è­˜è»¢ç§»
- [ ] **åˆ†æ•£å­¦ç¿’**: è¤‡æ•°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåŒæ™‚å­¦ç¿’
- [ ] **é©å¿œæœ€é©åŒ–**: å‹•çš„LoRAãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´

#### 2.2 é€²åŒ–çš„ç¾¤çŸ¥èƒ½å®Ÿè£…
- [ ] **éºä¼çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæˆ¦ç•¥é€²åŒ–
- [ ] **é›†å›£çŸ¥èƒ½**: ç¾¤ã‚Œè¡Œå‹•ãƒ»å‰µç™ºçš„å•é¡Œè§£æ±º
- [ ] **å¤šç›®çš„æœ€é©åŒ–**: ãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©è§£æ¢ç´¢
- [ ] **è‡ªå·±çµ„ç¹”åŒ–**: éšå±¤æ§‹é€ è‡ªå‹•å½¢æˆ

#### 2.3 å”èª¿çš„æœ€é©åŒ–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
- [ ] **å”èª¿å­¦ç¿’**: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“çŸ¥è­˜å…±æœ‰
- [ ] **ã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹**: åˆ†æ•£åˆæ„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
- [ ] **ãƒªã‚½ãƒ¼ã‚¹æœ€é©åŒ–**: è¨ˆç®—è³‡æºåŠ¹ç‡åŒ–
- [ ] **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**: å¤§è¦æ¨¡ã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œ

### Phase 3: ç ”ç©¶å¿œç”¨ï¼ˆé•·æœŸï¼‰

#### 3.1 å®Ÿä¸–ç•Œå•é¡Œé©ç”¨
- [ ] **æœ€é©åŒ–å•é¡Œ**: çµ„åˆã›æœ€é©åŒ–ãƒ»ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°
- [ ] **çµŒæ¸ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³**: å¸‚å ´ãƒ»ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³ãƒ»å–å¼•
- [ ] **ç¤¾ä¼šç§‘å­¦**: å”åŠ›è¡Œå‹•ãƒ»ç¤¾ä¼šè¦ç¯„å½¢æˆ
- [ ] **å·¥å­¦å¿œç”¨**: åˆ†æ•£åˆ¶å¾¡ãƒ»ãƒ­ãƒœãƒƒãƒˆç¾¤åˆ¶å¾¡

#### 3.2 å­¦è¡“è²¢çŒ®
- [ ] **è«–æ–‡åŸ·ç­†**: å›½éš›ä¼šè­°ãƒ»ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«æŠ•ç¨¿
- [ ] **ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**: æ€§èƒ½è©•ä¾¡åŸºæº–ç­–å®š
- [ ] **ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹**: ç ”ç©¶ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£è²¢çŒ®
- [ ] **ç”£æ¥­å¿œç”¨**: å®Ÿç”¨ã‚·ã‚¹ãƒ†ãƒ é–‹ç™º

## ğŸ”§ ãƒ†ã‚¹ãƒˆæˆ¦ç•¥

### å˜ä½“ãƒ†ã‚¹ãƒˆ
- [ ] **ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ©Ÿèƒ½**: å„æˆ¦ç•¥ã®æ­£ç¢ºæ€§æ¤œè¨¼
- [ ] **ã‚²ãƒ¼ãƒ å®Ÿè£…**: å ±é…¬è¨ˆç®—ãƒ»ãƒ«ãƒ¼ãƒ«éµå®ˆç¢ºèª
- [ ] **LLMçµ±åˆ**: APIå‘¼ã³å‡ºã—ãƒ»ãƒ¬ã‚¹ãƒãƒ³ã‚¹å‡¦ç†
- [ ] **ãƒ‡ãƒ¼ã‚¿å‡¦ç†**: åˆ†æãƒ»å¯è¦–åŒ–æ©Ÿèƒ½

### çµ±åˆãƒ†ã‚¹ãƒˆ
- [ ] **ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ç›¸äº’ä½œç”¨**: è¤‡é›‘ã‚·ãƒŠãƒªã‚ªæ¤œè¨¼
- [ ] **å®Ÿé¨“ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼**: end-to-endå®Ÿé¨“å®Ÿè¡Œ
- [ ] **æ€§èƒ½ãƒ†ã‚¹ãƒˆ**: å¤§è¦æ¨¡å®Ÿé¨“ãƒ»è² è·ãƒ†ã‚¹ãƒˆ
- [ ] **å›å¸°ãƒ†ã‚¹ãƒˆ**: æ©Ÿèƒ½æ›´æ–°æ™‚ã®å‹•ä½œä¿è¨¼

### æ¤œè¨¼å®Ÿé¨“
- [ ] **æ—¢çŸ¥çµæœå†ç¾**: ã‚²ãƒ¼ãƒ ç†è«–ç†è«–å€¤ã¨ã®æ¯”è¼ƒ
- [ ] **çµ±è¨ˆçš„æœ‰æ„æ€§**: ååˆ†ãªã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã§ã®æ¤œè¨¼
- [ ] **äº¤å·®æ¤œè¨¼**: ç•°ãªã‚‹æ¡ä»¶ã§ã®çµæœä¸€è²«æ€§
- [ ] **å°‚é–€å®¶è©•ä¾¡**: é ˜åŸŸå°‚é–€å®¶ã«ã‚ˆã‚‹çµæœæ¤œè¨¼

## ğŸ“ ç ”ç©¶ä¾¡å€¤

### å­¦è¡“çš„è²¢çŒ®
1. **æ–°è¦æ€§**: LangGraphÃ—LLMÃ—ã‚²ãƒ¼ãƒ ç†è«–ã®çµ±åˆ
2. **å®Ÿè¨¼æ€§**: çœŸã®AIæ¨è«–ã«ã‚ˆã‚‹æˆ¦ç•¥çš„ç›¸äº’ä½œç”¨
3. **æ–¹æ³•è«–**: ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿé¨“ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
4. **å¿œç”¨æ€§**: å®Ÿä¸–ç•Œå•é¡Œã¸ã®é©ç”¨å¯èƒ½æ€§

### æŠ€è¡“çš„é©æ–°
1. **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªåˆ†æ•£ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨­è¨ˆ
2. **çµ±åˆ**: ç•°ãªã‚‹AIæŠ€è¡“ã®åŠ¹æœçš„çµ„ã¿åˆã‚ã›
3. **è©•ä¾¡**: å®šé‡çš„ãƒ»å®šæ€§çš„åˆ†ææ‰‹æ³•
4. **åŠ¹ç‡æ€§**: è¨ˆç®—è³‡æºæœ€é©åŒ–æŠ€è¡“

## ğŸ“Š ç¾åœ¨ã®æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯

### é–‹ç™ºç’°å¢ƒ
- **Python**: 3.12.9
- **ä¾å­˜ç®¡ç†**: uv
- **IDE**: VS Code + Claude
- **OS**: WSL2 Ubuntu

### ä¸»è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
- **LangGraph**: ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç®¡ç†
- **OpenAI**: LLMçµ±åˆ
- **NumPy/Pandas**: ãƒ‡ãƒ¼ã‚¿å‡¦ç†
- **Matplotlib/Seaborn**: å¯è¦–åŒ–
- **Pytest**: ãƒ†ã‚¹ãƒ†ã‚£ãƒ³ã‚°

### ã‚¤ãƒ³ãƒ•ãƒ©
- **API**: OpenAI GPT-4o-mini
- **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸**: JSON/CSVå½¢å¼
- **ãƒ­ã‚°**: æ§‹é€ åŒ–ãƒ­ã‚°ç®¡ç†
- **è¨­å®š**: YAMLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

---

**æœ€çµ‚æ›´æ–°**: 2025å¹´6æœˆ21æ—¥
**é–‹ç™ºçŠ¶æ³**: åŸºç›¤ã‚·ã‚¹ãƒ†ãƒ å®Œæˆã€å®Ÿé¨“å®Ÿè¨¼æ¸ˆã¿ã€æ‹¡å¼µé–‹ç™ºæº–å‚™å®Œäº†