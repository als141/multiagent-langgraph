#!/usr/bin/env python3
"""æ—¥æœ¬èªLLMä¼šè©±ã«ã‚ˆã‚‹å¤šã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿé¨“"""

import asyncio
import sys
import json
import os
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import time

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from multiagent_system.agents import GameAgent
from multiagent_system.game_theory import Action
from multiagent_system.experiments.data_collector import DataCollector
from multiagent_system.utils import get_logger

# Import OpenAI
from openai import AsyncOpenAI

# Load environment variables
from dotenv import load_dotenv
load_dotenv()

logger = get_logger(__name__)


@dataclass
class ConversationContext:
    """LLMä¼šè©±ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ"""
    agent_name: str
    agent_strategy: str
    agent_personality: str
    opponent_name: str
    opponent_strategy: str
    interaction_history: List[Dict[str, Any]]
    current_situation: str
    game_history: List[Dict[str, Any]]


class JapaneseLLMConversationManager:
    """æ—¥æœ¬èªLLMä¼šè©±ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.client = AsyncOpenAI(
            api_key=os.getenv("OPENAI_API_KEY")
        )
        self.model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        self.temperature = float(os.getenv("OPENAI_TEMPERATURE", "0.7"))
        self.max_tokens = int(os.getenv("OPENAI_MAX_TOKENS", "1500"))
        
        logger.info(f"æ—¥æœ¬èªLLMç®¡ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†: {self.model}")
    
    async def generate_negotiation_message(self, context: ConversationContext) -> str:
        """äº¤æ¸‰ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ"""
        
        system_prompt = f"""ã‚ãªãŸã¯{context.agent_name}ã¨ã„ã†åå‰ã®AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ç‰¹å¾´ã‚’æŒã£ã¦ã„ã¾ã™ï¼š
- æˆ¦ç•¥: {context.agent_strategy}
- æ€§æ ¼: {context.agent_personality}

ã‚ãªãŸã¯{context.opponent_name}ï¼ˆæˆ¦ç•¥: {context.opponent_strategy}ï¼‰ã¨æˆ¦ç•¥çš„ç›¸äº’ä½œç”¨ã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚

ç›®æ¨™ã¯ã€ã‚ãªãŸã®æˆ¦ç•¥çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«å¿ å®Ÿã§ã‚ã‚ŠãªãŒã‚‰ã€ç›¸äº’åˆ©ç›Šã¨ãªã‚‹å”åŠ›åˆæ„ã‚’äº¤æ¸‰ã™ã‚‹ã“ã¨ã§ã™ã€‚
ä¼šè©±ã¯è‡ªç„¶ã§æˆ¦ç•¥çš„ã€ãã—ã¦ã‚ãªãŸã®æ€§æ ¼ã«å¿ å®Ÿã§ã‚ã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚
å›ç­”ã¯ç°¡æ½”ï¼ˆ1-2æ–‡ï¼‰ã§ã™ãŒæ„å‘³ã®ã‚ã‚‹ã‚‚ã®ã«ã—ã¦ãã ã•ã„ã€‚

ã™ã¹ã¦æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"""

        user_prompt = f"""ç¾åœ¨ã®çŠ¶æ³: {context.current_situation}

ã“ã‚Œã¾ã§ã®ç›¸äº’ä½œç”¨å±¥æ­´:
{self._format_history(context.interaction_history)}

{context.opponent_name}ã¸ã®äº¤æ¸‰ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚ä»¥ä¸‹ã‚’è€ƒæ…®ã—ã¦ãã ã•ã„ï¼š
1. ã‚ãªãŸã®æˆ¦ç•¥çš„ç›®æ¨™
2. ç›¸æ‰‹ã®æˆ¦ç•¥ã«åŸºã¥ãäºˆæƒ³ã•ã‚Œã‚‹åå¿œ
3. ä¿¡é ¼é–¢ä¿‚ã®æ§‹ç¯‰ã¾ãŸã¯ç¶­æŒ
4. ç›¸äº’åˆ©ç›Šã®å¯èƒ½æ€§

{context.agent_name}ã¨ã—ã¦å›ç­”:"""

        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )
            
            message = response.choices[0].message.content.strip()
            logger.debug(f"{context.agent_name}ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ: {message[:30]}...")
            return message
            
        except Exception as e:
            logger.error(f"{context.agent_name}ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return f"å”åŠ›ã«ã¤ã„ã¦è©±ã—åˆã„ã¾ã—ã‚‡ã†ã€‚"
    
    async def generate_response_message(
        self, 
        context: ConversationContext, 
        incoming_message: str
    ) -> Dict[str, Any]:
        """å—ä¿¡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¸ã®è¿”ç­”ã‚’ç”Ÿæˆ"""
        
        system_prompt = f"""ã‚ãªãŸã¯{context.agent_name}ã¨ã„ã†AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ï¼š
- æˆ¦ç•¥: {context.agent_strategy}
- æ€§æ ¼: {context.agent_personality}

{context.opponent_name}ã‹ã‚‰æ¬¡ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å—ã‘å–ã‚Šã¾ã—ãŸ: ã€Œ{incoming_message}ã€

ã‚ãªãŸã®æˆ¦ç•¥ã¨æ€§æ ¼ã«åŸºã¥ã„ã¦çœŸæ­£ãªåå¿œã‚’ã—ã¦ãã ã•ã„ã€‚
ã™ã¹ã¦æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"""

        user_prompt = f"""å—ä¿¡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’åˆ†æã—ã€ä»¥ä¸‹ã‚’æä¾›ã—ã¦ãã ã•ã„ï¼š
1. ã‚ãªãŸã®è¨€è‘‰ã«ã‚ˆã‚‹å›ç­”ï¼ˆ1-2æ–‡ï¼‰
2. å†…éƒ¨è©•ä¾¡ï¼ˆcooperation_likelihood: 0.0-1.0ï¼‰
3. ä¿¡é ¼åº¦å¤‰åŒ–ï¼ˆtrust_change: -0.5 to +0.5ï¼‰
4. ææ¡ˆã‚’å—ã‘å…¥ã‚Œã‚‹ã‹ï¼ˆaccept: true/falseï¼‰

JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
{{
    "response": "ã‚ãªãŸã®è¨€è‘‰ã«ã‚ˆã‚‹å›ç­”",
    "cooperation_likelihood": 0.0ã‹ã‚‰1.0,
    "trust_change": -0.5ã‹ã‚‰+0.5,
    "accept": true/false,
    "reasoning": "ç°¡æ½”ãªå†…éƒ¨çš„ãªæ¨è«–"
}}"""

        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )
            
            response_text = response.choices[0].message.content.strip()
            
            # JSONè§£æã‚’è©¦è¡Œ
            try:
                response_data = json.loads(response_text)
                logger.debug(f"{context.agent_name}ã®å›ç­”ç”Ÿæˆ: {response_data['response'][:20]}...")
                return response_data
            except json.JSONDecodeError:
                logger.warning(f"{context.agent_name}ã®JSONè§£æå¤±æ•—")
                return {
                    "response": response_text[:100],
                    "cooperation_likelihood": 0.5,
                    "trust_change": 0.0,
                    "accept": True,
                    "reasoning": "JSONè§£æã«å¤±æ•—ã—ã¾ã—ãŸ"
                }
                
        except Exception as e:
            logger.error(f"{context.agent_name}ã®å›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "response": "ã“ã®ææ¡ˆã«ã¤ã„ã¦è€ƒãˆã•ã›ã¦ãã ã•ã„ã€‚",
                "cooperation_likelihood": 0.5,
                "trust_change": 0.0,
                "accept": False,
                "reasoning": "APIã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"
            }
    
    async def generate_strategic_reflection(
        self, 
        context: ConversationContext,
        game_results: List[Dict[str, Any]]
    ) -> str:
        """æˆ¦ç•¥çš„æŒ¯ã‚Šè¿”ã‚Šã‚’ç”Ÿæˆ"""
        
        system_prompt = f"""ã‚ãªãŸã¯{context.agent_name}ã¨ã—ã¦æœ€è¿‘ã®æˆ¦ç•¥çš„ç›¸äº’ä½œç”¨ã«ã¤ã„ã¦æŒ¯ã‚Šè¿”ã£ã¦ã„ã¾ã™ã€‚
- ã‚ãªãŸã®æˆ¦ç•¥: {context.agent_strategy}
- ã‚ãªãŸã®æ€§æ ¼: {context.agent_personality}

æœ€è¿‘ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’åˆ†æã—ã€æ´å¯Ÿã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
ã™ã¹ã¦æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"""

        game_summary = self._format_game_results(game_results)
        
        user_prompt = f"""æœ€è¿‘ã®ã‚²ãƒ¼ãƒ çµæœ:
{game_summary}

ä»¥ä¸‹ã«ã¤ã„ã¦æŒ¯ã‚Šè¿”ã£ã¦ãã ã•ã„ï¼š
1. è¦³å¯Ÿã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³
2. ã‚ãªãŸã®æˆ¦ç•¥ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
3. ä»–ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã¤ã„ã¦å­¦ã‚“ã ã“ã¨
4. è€ƒæ…®ã—ã¦ã„ã‚‹æˆ¦ç•¥çš„èª¿æ•´

æ€æ…®æ·±ã„æŒ¯ã‚Šè¿”ã‚Šï¼ˆ2-3æ–‡ï¼‰ã‚’æä¾›ã—ã¦ãã ã•ã„ï¼š"""

        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )
            
            reflection = response.choices[0].message.content.strip()
            logger.debug(f"{context.agent_name}ã®æŒ¯ã‚Šè¿”ã‚Šç”Ÿæˆ: {reflection[:30]}...")
            return reflection
            
        except Exception as e:
            logger.error(f"{context.agent_name}ã®æŒ¯ã‚Šè¿”ã‚Šç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return "ã“ã‚Œã‚‰ã®çµæœã‚’ã‚ˆã‚Šæ³¨æ„æ·±ãåˆ†æã—ã¦æˆ¦ç•¥ã‚’æ”¹å–„ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"
    
    def _format_history(self, history: List[Dict[str, Any]]) -> str:
        """ç›¸äº’ä½œç”¨å±¥æ­´ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        if not history:
            return "ã“ã‚Œã¾ã§ã®ç›¸äº’ä½œç”¨ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
        
        formatted = []
        for item in history[-3:]:  # æœ€æ–°3ä»¶
            formatted.append(f"- {item.get('type', 'ç›¸äº’ä½œç”¨')}: {item.get('summary', 'N/A')}")
        
        return "\n".join(formatted)
    
    def _format_game_results(self, results: List[Dict[str, Any]]) -> str:
        """ã‚²ãƒ¼ãƒ çµæœã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        if not results:
            return "ã‚²ãƒ¼ãƒ çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
        
        formatted = []
        for result in results[-5:]:  # æœ€æ–°5ä»¶
            my_action = result.get('my_action', 'ä¸æ˜')
            opponent_action = result.get('opponent_action', 'ä¸æ˜')
            payoff = result.get('my_payoff', 0)
            action_jp = "å”åŠ›" if my_action == "cooperate" else "è£åˆ‡ã‚Š"
            opp_action_jp = "å”åŠ›" if opponent_action == "cooperate" else "è£åˆ‡ã‚Š"
            formatted.append(f"- ç§: {action_jp}, ç›¸æ‰‹: {opp_action_jp}, ç§ã®åˆ©å¾—: {payoff}")
        
        return "\n".join(formatted)


async def run_japanese_llm_experiment():
    """æ—¥æœ¬èªLLMä¼šè©±å®Ÿé¨“ã‚’å®Ÿè¡Œ"""
    
    print("ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªLLMãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä¼šè©±å®Ÿé¨“")
    print("=" * 50)
    
    # æ—¥æœ¬èªLLMä¼šè©±ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–
    llm_manager = JapaneseLLMConversationManager()
    
    # æ—¥æœ¬èªã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨­å®šã‚’ä½œæˆ
    agent_configs = [
        {
            "name": "å¤–äº¤å®˜_ç”°ä¸­",
            "strategy": "tit_for_tat",
            "personality": "ç¤¼å„€æ­£ã—ãç›¸äº’åˆ©ç›Šã‚’é‡è¦–ã™ã‚‹å¤–äº¤å®˜ã€‚é•·æœŸçš„ãªé–¢ä¿‚æ§‹ç¯‰ã‚’å¤§åˆ‡ã«ã—ã€ç›¸æ‰‹ã®è¡Œå‹•ã«å¿œã˜ã¦å¯¾å¿œã‚’å¤‰ãˆã‚‹æˆ¦ç•¥å®¶ã€‚",
            "specialization": "å¤–äº¤å°‚é–€å®¶"
        },
        {
            "name": "æ¥½è¦³ä¸»ç¾©è€…_ä½è—¤", 
            "strategy": "always_cooperate",
            "personality": "å¸¸ã«å‰å‘ãã§ä»–è€…ã‚’ä¿¡é ¼ã™ã‚‹å”åŠ›ä¸»ç¾©è€…ã€‚èª°ã¨ã§ã‚‚å”åŠ›ã§ãã‚‹ã¨ä¿¡ã˜ã€ãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®ä¾¡å€¤ã‚’é‡è¦–ã™ã‚‹ã€‚",
            "specialization": "å”åŠ›å°‚é–€å®¶"
        },
        {
            "name": "æˆ¦ç•¥å®¶_éˆ´æœ¨",
            "strategy": "always_defect", 
            "personality": "å†·é™ã§åˆç†çš„ãªåˆ¤æ–­ã‚’ä¸‹ã™æˆ¦ç•¥åˆ†æè€…ã€‚è‡ªå·±åˆ©ç›Šã‚’æœ€å„ªå…ˆã«è€ƒãˆã€åŠ¹ç‡çš„ãªçµæœã‚’æ±‚ã‚ã‚‹ç¾å®Ÿä¸»ç¾©è€…ã€‚",
            "specialization": "æˆ¦ç•¥åˆ†æå°‚é–€å®¶"
        },
        {
            "name": "é©å¿œè€…_å±±ç”°",
            "strategy": "adaptive_tit_for_tat",
            "personality": "çŠ¶æ³ã‚’è¦³å¯Ÿã—å­¦ç¿’ã™ã‚‹é©å¿œå‹æ€è€ƒè€…ã€‚ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æã—ã€æœ€é©ãªæˆ¦ç•¥ã‚’å‹•çš„ã«èª¿æ•´ã™ã‚‹æŸ”è»Ÿæ€§ã‚’æŒã¤ã€‚",
            "specialization": "ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æå°‚é–€å®¶"
        }
    ]
    
    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ
    agents = []
    for config in agent_configs:
        agent = GameAgent(
            name=config["name"],
            strategy_name=config["strategy"],
            specialization=config["specialization"]
        )
        agent.llm_personality = config["personality"]
        agents.append(agent)
        print(f"âœ… {agent.name} ({agent.strategy.name}) - {config['personality'][:40]}...")
    
    # ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹
    collector = DataCollector("japanese_llm_experiment", ["all"])
    collector.start_collection("japanese_run_001", {"language": "japanese", "llm_enabled": True}, agents)
    
    print(f"\nğŸ—£ï¸ æ—¥æœ¬èªLLMä¼šè©±ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    print("=" * 30)
    
    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå±¥æ­´è¿½è·¡
    agent_histories = {agent.agent_id: [] for agent in agents}
    all_interactions = []
    
    # ã‚·ãƒŠãƒªã‚ª1: ç”°ä¸­ï¼ˆå¤–äº¤å®˜ï¼‰ã¨ä½è—¤ï¼ˆæ¥½è¦³ä¸»ç¾©è€…ï¼‰ã®å”åŠ›äº¤æ¸‰
    print(f"\nğŸ¤ ã‚·ãƒŠãƒªã‚ª1: å¤–äº¤çš„å”åŠ›äº¤æ¸‰")
    print("-" * 30)
    
    tanaka = agents[0]  # å¤–äº¤å®˜_ç”°ä¸­
    sato = agents[1]    # æ¥½è¦³ä¸»ç¾©è€…_ä½è—¤
    
    # ç”°ä¸­ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ
    tanaka_context = ConversationContext(
        agent_name=tanaka.name,
        agent_strategy=tanaka.strategy.name,
        agent_personality=tanaka.llm_personality,
        opponent_name=sato.name,
        opponent_strategy=sato.strategy.name,
        interaction_history=agent_histories[tanaka.agent_id],
        current_situation="åˆå›å”åŠ›åˆæ„ã®ãŸã‚ã®äº¤æ¸‰",
        game_history=[]
    )
    
    # ç”°ä¸­ãŒäº¤æ¸‰ã‚’é–‹å§‹
    print("ğŸ’­ ç”°ä¸­ã®äº¤æ¸‰é–‹å§‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆä¸­...")
    tanaka_message = await llm_manager.generate_negotiation_message(tanaka_context)
    print(f"ğŸ—£ï¸ {tanaka.name}: ã€Œ{tanaka_message}ã€")
    
    # ä½è—¤ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ
    sato_context = ConversationContext(
        agent_name=sato.name,
        agent_strategy=sato.strategy.name,
        agent_personality=sato.llm_personality,
        opponent_name=tanaka.name,
        opponent_strategy=tanaka.strategy.name,
        interaction_history=agent_histories[sato.agent_id],
        current_situation="ç”°ä¸­ã®äº¤æ¸‰ææ¡ˆã¸ã®å¯¾å¿œ",
        game_history=[]
    )
    
    # ä½è—¤ãŒå¿œç­”
    print("ğŸ’­ ä½è—¤ã®è¿”ç­”ã‚’ç”Ÿæˆä¸­...")
    sato_response = await llm_manager.generate_response_message(sato_context, tanaka_message)
    print(f"ğŸ—£ï¸ {sato.name}: ã€Œ{sato_response['response']}ã€")
    print(f"   ğŸ“Š å†…éƒ¨è©•ä¾¡: å”åŠ›å¯èƒ½æ€§ {sato_response['cooperation_likelihood']:.2f}, ä¿¡é ¼å¤‰åŒ– {sato_response['trust_change']:+.2f}")
    
    # ç›¸äº’ä½œç”¨ã‚’ãƒ­ã‚°
    collector.collect_interaction_data(
        interaction_type="japanese_negotiation",
        participants=[tanaka.agent_id, sato.agent_id],
        details={
            "initiator": tanaka.agent_id,
            "initiator_message": tanaka_message,
            "target": sato.agent_id,
            "target_response": sato_response['response'],
            "target_assessment": sato_response,
            "language": "japanese",
            "llm_generated": True
        }
    )
    
    # å±¥æ­´æ›´æ–°
    agent_histories[tanaka.agent_id].append({
        "type": "negotiation_sent", 
        "summary": f"{sato.name}ã«å”åŠ›ã‚’ææ¡ˆ"
    })
    agent_histories[sato.agent_id].append({
        "type": "negotiation_received",
        "summary": f"{tanaka.name}ã‹ã‚‰å”åŠ›ææ¡ˆã‚’å—é ˜ã€å¥½æ„çš„ã«è©•ä¾¡"
    })
    
    # ã‚·ãƒŠãƒªã‚ª2: éˆ´æœ¨ï¼ˆæˆ¦ç•¥å®¶ï¼‰ã¨å±±ç”°ï¼ˆé©å¿œè€…ï¼‰ã®æˆ¦ç•¥çš„å¯¾è©±
    print(f"\nâš”ï¸ ã‚·ãƒŠãƒªã‚ª2: æˆ¦ç•¥çš„åˆ©å®³å¯¾ç«‹")
    print("-" * 30)
    
    suzuki = agents[2]  # æˆ¦ç•¥å®¶_éˆ´æœ¨
    yamada = agents[3]  # é©å¿œè€…_å±±ç”°
    
    # éˆ´æœ¨ãŒåˆ©å·±çš„ææ¡ˆã‚’é–‹å§‹
    suzuki_context = ConversationContext(
        agent_name=suzuki.name,
        agent_strategy=suzuki.strategy.name,
        agent_personality=suzuki.llm_personality,
        opponent_name=yamada.name,
        opponent_strategy=yamada.strategy.name,
        interaction_history=agent_histories[suzuki.agent_id],
        current_situation="ä¸»ã«è‡ªåˆ†ã«æœ‰åˆ©ãªæˆ¦ç•¥çš„ææ¡ˆã‚’è¡Œã†",
        game_history=[]
    )
    
    print("ğŸ’­ éˆ´æœ¨ã®æˆ¦ç•¥çš„ææ¡ˆã‚’ç”Ÿæˆä¸­...")
    suzuki_message = await llm_manager.generate_negotiation_message(suzuki_context)
    print(f"ğŸ—£ï¸ {suzuki.name}: ã€Œ{suzuki_message}ã€")
    
    # å±±ç”°ã®åˆ†æçš„å¿œç­”
    yamada_context = ConversationContext(
        agent_name=yamada.name,
        agent_strategy=yamada.strategy.name,
        agent_personality=yamada.llm_personality,
        opponent_name=suzuki.name,
        opponent_strategy=suzuki.strategy.name,
        interaction_history=agent_histories[yamada.agent_id],
        current_situation="éˆ´æœ¨ã®åˆ©å·±çš„ææ¡ˆã‚’åˆ†æä¸­",
        game_history=[]
    )
    
    print("ğŸ’­ å±±ç”°ã®åˆ†æçš„å¿œç­”ã‚’ç”Ÿæˆä¸­...")
    yamada_response = await llm_manager.generate_response_message(yamada_context, suzuki_message)
    print(f"ğŸ—£ï¸ {yamada.name}: ã€Œ{yamada_response['response']}ã€")
    print(f"   ğŸ“Š å†…éƒ¨è©•ä¾¡: å”åŠ›å¯èƒ½æ€§ {yamada_response['cooperation_likelihood']:.2f}, ä¿¡é ¼å¤‰åŒ– {yamada_response['trust_change']:+.2f}")
    
    # ç›¸äº’ä½œç”¨ã‚’ãƒ­ã‚°
    collector.collect_interaction_data(
        interaction_type="japanese_confrontation",
        participants=[suzuki.agent_id, yamada.agent_id],
        details={
            "initiator": suzuki.agent_id,
            "initiator_message": suzuki_message,
            "target": yamada.agent_id,
            "target_response": yamada_response['response'],
            "target_assessment": yamada_response,
            "language": "japanese",
            "llm_generated": True
        }
    )
    
    # ã‚·ãƒŠãƒªã‚ª3: å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æˆ¦ç•¥çš„æŒ¯ã‚Šè¿”ã‚Š
    print(f"\nğŸ§  ã‚·ãƒŠãƒªã‚ª3: æˆ¦ç•¥çš„æŒ¯ã‚Šè¿”ã‚Šã‚»ãƒƒã‚·ãƒ§ãƒ³")
    print("-" * 30)
    
    # ã‚²ãƒ¼ãƒ çµæœã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    mock_game_results = [
        {"my_action": "cooperate", "opponent_action": "cooperate", "my_payoff": 3.0},
        {"my_action": "cooperate", "opponent_action": "defect", "my_payoff": 0.0},
        {"my_action": "defect", "opponent_action": "cooperate", "my_payoff": 5.0},
    ]
    
    print("ğŸ’­ å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æˆ¦ç•¥çš„æŒ¯ã‚Šè¿”ã‚Šã‚’ç”Ÿæˆä¸­...")
    
    reflections = {}
    for agent in agents:
        context = ConversationContext(
            agent_name=agent.name,
            agent_strategy=agent.strategy.name,
            agent_personality=agent.llm_personality,
            opponent_name="æ§˜ã€…ãªç›¸æ‰‹",
            opponent_strategy="æ··åˆæˆ¦ç•¥",
            interaction_history=agent_histories[agent.agent_id],
            current_situation="æœ€è¿‘ã®æˆ¦ç•¥çš„ç›¸äº’ä½œç”¨ã®æŒ¯ã‚Šè¿”ã‚Š",
            game_history=mock_game_results
        )
        
        reflection = await llm_manager.generate_strategic_reflection(context, mock_game_results)
        reflections[agent.name] = reflection
        print(f"ğŸ¤” {agent.name}: ã€Œ{reflection}ã€")
        print()
    
    # ã‚°ãƒ«ãƒ¼ãƒ—æŒ¯ã‚Šè¿”ã‚Šã‚’ãƒ­ã‚°
    collector.collect_interaction_data(
        interaction_type="japanese_group_reflection",
        participants=[agent.agent_id for agent in agents],
        details={
            "reflection_topic": "æˆ¦ç•¥çš„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ",
            "reflections": reflections,
            "game_context": mock_game_results,
            "language": "japanese",
            "llm_generated": True
        }
    )
    
    # ã‚·ãƒŠãƒªã‚ª4: ç”°ä¸­ã¨å±±ç”°ã®çŸ¥è­˜äº¤æ›
    print(f"\nğŸ’¡ ã‚·ãƒŠãƒªã‚ª4: å°‚é–€çŸ¥è­˜ã®ç›¸äº’äº¤æ›")
    print("-" * 30)
    
    # ç”°ä¸­ãŒæ´å¯Ÿã‚’å…±æœ‰
    print("ğŸ’­ ç”°ä¸­ã®çŸ¥è­˜å…±æœ‰ã‚’ç”Ÿæˆä¸­...")
    tanaka_updated_context = ConversationContext(
        agent_name=tanaka.name,
        agent_strategy=tanaka.strategy.name,
        agent_personality=tanaka.llm_personality,
        opponent_name=yamada.name,
        opponent_strategy=yamada.strategy.name,
        interaction_history=agent_histories[tanaka.agent_id],
        current_situation="æˆ¦ç•¥çš„æ´å¯Ÿã¨å­¦ç¿’ã—ãŸæ•™è¨“ã®å…±æœ‰",
        game_history=mock_game_results
    )
    
    tanaka_insight = await llm_manager.generate_negotiation_message(tanaka_updated_context)
    print(f"ğŸ”¬ {tanaka.name}ãŒå…±æœ‰: ã€Œ{tanaka_insight}ã€")
    
    # å±±ç”°ãŒçŸ¥è­˜ã§å¿œç­”
    print("ğŸ’­ å±±ç”°ã®çŸ¥è­˜äº¤æ›å¿œç­”ã‚’ç”Ÿæˆä¸­...")
    yamada_insight_response = await llm_manager.generate_response_message(
        yamada_context, 
        f"ç”°ä¸­ãŒã“ã®æ´å¯Ÿã‚’å…±æœ‰ã—ã¾ã—ãŸ: {tanaka_insight}"
    )
    print(f"ğŸ”¬ {yamada.name}ãŒå¿œç­”: ã€Œ{yamada_insight_response['response']}ã€")
    
    # çŸ¥è­˜äº¤æ›ã‚’ãƒ­ã‚°
    collector.collect_interaction_data(
        interaction_type="japanese_knowledge_exchange",
        participants=[tanaka.agent_id, yamada.agent_id],
        details={
            "knowledge_shared": tanaka_insight,
            "response_insight": yamada_insight_response['response'],
            "exchange_quality": "high",
            "mutual_learning": True,
            "language": "japanese",
            "llm_generated": True
        }
    )
    
    print(f"\nğŸ“Š å®Ÿé¨“åˆ†æ")
    print("=" * 15)
    
    # ãƒ‡ãƒ¼ã‚¿åé›†ã®çµ‚äº†
    class MockCoordinator:
        def __init__(self):
            self.round_counter = 4
            
        def get_coordination_summary(self):
            return {
                "final_round": self.round_counter,
                "total_agents": len(agents),
                "language": "japanese",
                "llm_enabled": True
            }
    
    mock_coordinator = MockCoordinator()
    
    experimental_data = collector.finalize_collection(
        final_state={"round_number": 4, "japanese_llm_experiment_complete": True},
        agents=agents,
        coordinator=mock_coordinator
    )
    
    print(f"ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªLLMç›¸äº’ä½œç”¨: {len(experimental_data.interaction_logs)}")
    print(f"ğŸ’¬ LLMç”Ÿæˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {sum(1 for i in experimental_data.interaction_logs if i.get('details', {}).get('llm_generated', False))}")
    print(f"ğŸ§  æˆ¦ç•¥çš„æŒ¯ã‚Šè¿”ã‚Š: {len([i for i in experimental_data.interaction_logs if 'reflection' in i.get('type', '')])}")
    print(f"ğŸ”„ çŸ¥è­˜äº¤æ›: {len([i for i in experimental_data.interaction_logs if 'knowledge' in i.get('type', '')])}")
    
    # è©³ç´°çµæœã‚’ä¿å­˜
    japanese_results = {
        "å®Ÿé¨“æ¦‚è¦": {
            "llm_model": llm_manager.model,
            "è¨€èª": "æ—¥æœ¬èª",
            "ç·ç›¸äº’ä½œç”¨æ•°": len(experimental_data.interaction_logs),
            "ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ": [
                {
                    "åå‰": agent.name,
                    "æˆ¦ç•¥": agent.strategy.name,
                    "æ€§æ ¼": agent.llm_personality
                }
                for agent in agents
            ]
        },
        "æ—¥æœ¬èªä¼šè©±": [
            {
                "ã‚¿ã‚¤ãƒ—": interaction.get("type"),
                "å‚åŠ è€…": [
                    next(agent.name for agent in agents if agent.agent_id == pid)
                    for pid in interaction.get("participants", [])
                ],
                "è©³ç´°": interaction.get("details", {})
            }
            for interaction in experimental_data.interaction_logs
        ],
        "æˆ¦ç•¥çš„æ´å¯Ÿ": {
            "å¤–äº¤çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ": "ç”°ä¸­ã¯é–¢ä¿‚æ§‹ç¯‰ã«ç„¦ç‚¹ã‚’å½“ã¦ãŸæ´—ç·´ã•ã‚ŒãŸäº¤æ¸‰ã‚’å®Ÿè¨¼",
            "æ¥½è¦³çš„å”åŠ›": "ä½è—¤ã¯é«˜ã„å”åŠ›å¯èƒ½æ€§ã§ä¸€è²«ã—ãŸå‰å‘ããªåå¿œã‚’ç¤ºã—ãŸ",
            "è¨ˆç®—çš„æˆ¦ç•¥": "éˆ´æœ¨ã¯æ˜ç¢ºãªè‡ªå·±åˆ©ç›Šæœ€å¤§åŒ–ã®è¦–ç‚¹ã‚’è¡¨æ˜",
            "é©å¿œçš„å­¦ç¿’": "å±±ç”°ã¯åˆ†æçš„ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ã¨æˆ¦ç•¥çš„èª¿æ•´ã‚’ç¤ºã—ãŸ"
        }
    }
    
    # çµæœä¿å­˜
    results_file = Path("japanese_llm_results.json")
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(japanese_results, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\nğŸ’¾ å®Œå…¨ãªæ—¥æœ¬èªLLMä¼šè©±çµæœä¿å­˜: {results_file}")
    
    # ä¸»è¦ãªæ—¥æœ¬èªç›¸äº’ä½œç”¨ã‚’è¡¨ç¤º
    print(f"\nğŸ¯ ä¸»è¦ãªLLMç”Ÿæˆæ´å¯Ÿï¼ˆæ—¥æœ¬èªï¼‰")
    print("=" * 25)
    
    for i, interaction in enumerate(experimental_data.interaction_logs):
        if interaction.get('details', {}).get('llm_generated'):
            details = interaction['details']
            print(f"\nğŸ’¬ ç›¸äº’ä½œç”¨ {i+1}: {interaction['type'].upper()}")
            if 'initiator_message' in details:
                print(f"   ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: ã€Œ{details['initiator_message'][:60]}...ã€")
            if 'target_response' in details:
                print(f"   å¿œç­”: ã€Œ{details['target_response'][:60]}...ã€")
    
    print(f"\nğŸ‰ æ—¥æœ¬èªLLMå®Ÿé¨“ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
    return True


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    # OpenAI APIã‚­ãƒ¼ã®ç¢ºèª
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ OPENAI_API_KEYãŒç’°å¢ƒå¤‰æ•°ã«ã‚ã‚Šã¾ã›ã‚“ï¼")
        print("   .envãƒ•ã‚¡ã‚¤ãƒ«ã«OpenAI APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„")
        return False
    
    print(f"ğŸ”‘ OpenAI APIã‚­ãƒ¼è¨­å®šæ¸ˆã¿: {os.getenv('OPENAI_API_KEY')[:10]}...")
    
    try:
        success = await run_japanese_llm_experiment()
        if success:
            print("âœ… å…¨ã¦ã®æ—¥æœ¬èªLLMç›¸äº’ä½œç”¨ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
        else:
            print("âŒ ä¸€éƒ¨ã®æ—¥æœ¬èªLLMç›¸äº’ä½œç”¨ãŒå¤±æ•—ã—ã¾ã—ãŸã€‚")
    except Exception as e:
        print(f"âŒ å®Ÿé¨“å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())